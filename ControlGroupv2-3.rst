`Control Group v2 <https://docs.kernel.org/admin-guide/cgroup-v2.html>`_  の非公式日本語訳です。
ライセンスは原文のライセンス(GPL-2.0のはず)に従います。

訳注: 

* まだDeepLの結果をはる途中(2024/3/10)
* ファイルがでかすぎるようなのでファイル分割(2024/3/9)

IO
--

io」コントローラーは、IOリソースの分配を制御する。 このコントローラは、ウェイトベースの分配と、絶対的な帯域幅またはIOPS制限の分配の両方を実装している。blk-mqデバイスではどちらのスキームも利用できません。

IO インターフェースファイル
~~~~~~~~~~~~~~~~~~

 io.stat
	読み取り専用のネスト・キー・ファイル。

	行は$MAJ:$MINデバイス番号でキーが付けられ、順序は付けられない。
	以下のネストされたキーが定義されている。

	  ====== =====================
	  rbytes 読み込まれたバイト数
	  wbytes 書き込まれたバイト数
	  rios 読み込みIO数
	  wios 書き込み IO 数
	  dbytes 廃棄されたバイト数
	  dios 廃棄IO数
	  ====== =====================  

	  

	読み取り出力の例を以下に示す::

	  8:16 rbytes=1459200 wbytes=314773504 rios=192 wios=353 dbytes=0 dios=0
	  8:0 rbytes=90430464 wbytes=299008000 rios=8950 wios=1252 dbytes=50331648 dios=3021

  io.cost.qos
	rootにのみ存在する、読み書き可能なネストされたキー付きファイル。
	cgroupにのみ存在する。

	このファイルは、IOコストモデルベースのコントローラ(CONFIG_COST.qos)のサービス品質を設定します。
	モデルベースのコントローラ (CONFIG_BLK_CGROUP_IOCOST) のサービス品質を設定します。
	現在、「io.weight」比例制御を実装している。 行
	は、$MAJ:$MIN デバイス番号によってキー設定され、順序付けされていない。 並び順ではありません。
	行は、$MAJ:$MINのデバイス番号でキー設定され、順序はありません。
	行は、"io.cost.qos" または "io.cost.model "上のデバイスに対する最初の書き込み時に入力される。 以下の
	以下のネストされたキーが定義されている。

	  ====== =====================================
	  enable ウェイト制御 enable
	  ctrl "auto" または "user"
	  rpct 読み取り遅延パーセンタイル [0, 100］
	  rlat 読み出しレイテンシ閾値
	  wpct 書き込み遅延パーセンタイル [0, 100］
	  wlat 書き込み遅延のしきい値
	  min 最小スケーリングパーセンテージ [1, 10000］
	  max 最大スケーリング・パーセンテージ [1, 10000］
	  ====== =====================================

	コントローラーはデフォルトでは無効であり、enableを1に設定することで有効になる。
	enable "を "1 "に設定することで有効にできる。
	をゼロに設定し、コントローラは内部デバイスの飽和状態
	状態を使用して、全体のIOレートを "min "から "max "の間で調整する。

	より良い制御品質が必要な場合は、レイテンシQoS
	パラメータを設定することができる。 例えば、以下のようになる::

	  8:16 enable=1 ctrl=auto rpct=95.00 rlat=75000 wpct=95.00 wlat=150000 min=50.00 max=150.0

	は、sdbでコントローラーが有効になっていることを示す。
	デバイスが飽和したとみなす。
	レイテンシの95パーセンタイルが75msまたは150msを超えると、デバイスが飽和したとみなし、全体の
	を調整する。	飽和ポイントが低いほど、集約帯域幅を犠牲にして、レイテンシQoSが向上する。
	が向上する。 許容される
	最小 "と "最大 "の間の調整範囲が狭ければ狭いほど、IOの動作はコストモデル
	IOの動作がコストモデルに適合する。 IO問題
	基本レートが100%から大きく外れている可能性があり、"min "と "max "をやみくもに設定すると
	やみくもに "min "と "max "を設定すると、デバイスの容量や制御品質が著しく損なわれることがある。
	制御品質を著しく損なうことになる。 「min "と "max "は、以下のようなデバイスを制御するのに有用である。
	一時的に動作が大きく変化するデバイスを調整するのに便利です。
	ssdがしばらくの間ラインスピードで書き込みを受け付け、その後数秒間完全にストールする。
	その後何秒間も完全に停止する。

	以上から、内蔵の線形モデルは、逐次IOとランダムIOの基本コストとコスト係数を決定する。
	逐次IOとランダムIOの基本コストと、IOサイズに対するコスト係数
	を決定する。 単純ではあるが、このモデルは一般的なデバイスクラスをカバーできる。

	IOコストモデルは、絶対的な意味で正確であることは期待されていません。
	デバイスの動作に合わせて動的にスケーリングされる。

	必要であれば、tools/cgroup/iocost_coef_gen.py を使用して、デバイス固有の係数を生成できます。
	デバイス固有の係数を生成します。

  io.weight
	非 root cgroup に存在する読み書き可能なフラットキーファイル。
	デフォルトは "default 100 "です。

        最初の行はデバイスに適用されるデフォルトのウェイトです。
	残りの行は
	MAJ:$MINデバイス番号でキー設定されたオーバーライドで、順序はありません。 ウェイトは
	1, 10000] の範囲で、cgroup が使用できる相対的な IO 時間を指定します。
	を指定します。

	デフォルトの重みは、"default" または単に "$WEIGHT" と書くことで更新できる。 オーバーライドは
	「MAJ:$MIN$WEIGHT "と書くことで設定でき、"$MAJ:$MIN default "と書くことで解除できる。

	読み取り出力の例は以下の通り::

	  デフォルト 100
	  8:16 200
	  8:0 50

  io.max
	非ルートのファイル。

	BPSとIOPSベースのIO制限。 行は $MAJ:$MIN
	デバイス番号でキー付けされ、順序付けされていない。 以下のネストされたキーが定義されている。
	定義されている。

	  ===== ==================================
	  rbps 1秒あたりの最大読み取りバイト数
	  wbps 1 秒あたりの最大書き込みバイト数
	  riops 1 秒あたりの最大読み取り IO 操作数
	  wiops 最大書き込み IO
	  ===== ==================================

	書くときには、入れ子になったキーと値のペアをいくつでも、任意の順序で指定できる。
	を任意の順序で指定できる。 値として "max "を指定できます。
	を指定することができる。 同じキーが
	が複数回指定された場合、結果は未定義である。

	BPSとIOPSは各IO方向で測定され、IOは制限に達すると遅延される。
	は遅延される。 一時的なバーストは許される。

	8:16:の読み込み制限を2M BPS、書き込み制限を120 IOPSに設定する::

	  echo "8:16 rbps=2097152 wiops=120" > io.max

  読み込みは以下を返す::

	  8:16 rbps=2097152 wbps=max riops=max wiops=120

	書き込みIOPS制限は、次のように書くことで解除できる：

	  echo "8:16 wiops=max" > io.max

	読み込むと次のようになる：

	  8:16 rbps=2097152 wbps=max riops=max wiops=max

  io.pressure
	読み取り専用のネストされたキーファイル。

	IO の圧力失速情報を示す。参照
	詳細は :ref:`Documentation/accounting/psi.rst <psi>` を参照のこと。

Writeback
~~~~~~~~~

ページ・キャッシュはバッファード・ライトと共有mmapによって汚され、ライトバックによって非同期にバッキング・ファイルシステムに書き込まれる。
ライトバック・メカニズムによって非同期にバッキング・ファイルシステムに書き込まれる。
メカニズムによってバッキング・ファイルシステムに非同期に書き込まれる。 ライトバックはメモリとIOドメインの間に位置し
ダーティメモリの割合を調整する。
IOを書き込む。

IOコントローラは、メモリコントローラと連携して、ページキャッシュのライトバック制御を実装している、
ページキャッシュのライトバックIOの制御を実装している。 メモリコントローラ
メモリコントローラは、ダーティメモリ比率が計算され、維持されるメモリドメインを定義します。
メモリ・コントローラは、ダーティ・メモリ比率が計算され維持されるメモリ・ドメインを定義し、ioコントローラは
を定義する。 システム全体と
ダーティ・メモリの状態は、システム全体とグループごとに検査され、より制限の厳しい方
が強制される。

cgroupライトバックには、基礎となるファイルシステムからの明示的なサポートが必要です。 現在、cgroup writeback は ext2、ext4、btrfs、f2fs、および xfs で実装されています、
btrfs、f2fs、および xfs で実装されています。 その他のファイルシステムでは、すべてのライトバック IO は 
に帰属します。

メモリとライトバックの管理には固有の違いがあります。
に固有の違いがあり、cgroup の所有権の追跡方法に影響します。 メモリは
ページ単位で追跡されます。 ライトバックでは
inodeはcgroupに割り当てられ、inodeからダーティページを書き込むすべてのIOリクエストは
を書き込むすべての IO リクエストはその cgroup に帰属する。

メモリのcgroup所有権はページごとに追跡されるため、異なるcgroupに関連付けられたページが存在する可能性がある。
ページが存在する可能性があります。
とは異なるcgroupに関連付けられたページが存在することがある。 これらは外部ページと呼ばれる。 ライトバック
は常に

このモデルは、指定されたinodeが単一のcgroupによってほとんど汚されるようなほとんどのユースケースには十分である。
が単一のcgroupによってほとんど汚されるような使用例では、このモデルで十分である。
が変更されても、特定の inode が単一の cgroup によってほとんど汚されるような使用例では、このモデルで十分です。
に同時に書き込むような使用例はうまくサポートされません。 このような状況では
IOのかなりの部分が誤って帰属する可能性が高い。
メモリコントローラは最初の使用時にページの所有権を割り当て
ページが解放されるまで更新されないため、ライトバックがページ所有権に厳密に従うとしても
がページ所有権に厳密に従ったとしても、複数のcgroupが重複する領域
領域をダーティにする複数の  このような
パターンを避けることを推奨する。

ライトバックの動作に影響する sysctl ノブは、cgroup
ライトバックに適用されます。

  vm.dirty_background_ratio, vm.dirty_ratio
	これらの比率は、cgroupライトバックにも同じように適用されます。
	これらの比率はcgroup writebackにも適用されます。
	メモリコントローラとシステム全体のクリーンメモリによって制限されます。

  vm.dirty_background_bytes, vm.dirty_bytes
	cgroupライトバックの場合、これは次のように計算されます。
	と同じ方法で適用されます。
	vm.dirty[_background]_ratio と同じ方法で適用されます。

IO Latency
~~~~~~~~~~
これは、IO ワークロード保護用の cgroup v2 コントローラです。 グループに
にレイテンシターゲットを指定し、平均レイテンシがそのターゲットを超えると
コントローラは、保護されたワークロードよりも低いレイテンシターゲットを持つピアをスロットルします。
をスロットルします。

制限は、階層内のピアレベルでのみ適用されます。 つまり
下図では、グループA、B、Cのみが互いに影響し合い、グループDとFは互いに影響し合います。
グループ D と F は互いに影響し合います。 グループGは誰にも影響しません::


			[root]
		/	   |		\
		A	   B		C
	       /  \        |
	      D    F	   G


つまり、io.latencyをグループA、B、Cに設定するのが理想的な設定方法です。
一般的に、デバイスがサポートするレイテンシよりも低い値を設定することは避けたいものです。
をサポートするレイテンシーより低い値には設定したくないものです。 ワークロードに最適な値を見つけるために実験してください。
お使いのデバイスで予想されるレイテンシよりも高い値から始めて
io.statのワークロードグループのavg_lat値を見て、通常動作時のレイテンシを把握する。
のio.statのavg_lat値を見る。 このavg_lat値を、実際の設定の基準として使用する。
io.statの値より10～15%高く設定する。

IOレイテンシ・スロットリングの仕組み
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

io.latencyは仕事を節約する。
を満たしている限り、コントローラは何もしない。 一旦グループがその目標を逃し始めると
一旦グループがそのターゲットを逃し始めると、自分よりも高いターゲットを持つピアグループをスロットリングし始める。
このスロットリングには2つの形式があります：

- キューの深さのスロットリング。 これは、グループが持つことを許される未処理のIOの数である。
  の数です。 制限なしから始めて、一度に1IOまで、比較的早く制限する。
  から始まり、一度に1IOに至るまで。

- 人工的な遅延誘導。 ある種のIOは
  優先順位の高いグループに悪影響を与えない限り、スロットルできないIOがある。 これには
  これにはスワッピングとメタデータIOが含まれる。 これらのタイプのIOは、通常
  しかし、これらは発生元グループに「課金」される。 もし
  発信元グループがスロットルされている場合、io.statのuse_delayとdelay
  フィールドが増加する。 delay値は、io.statのuse_delayフィールドとdelayフィールドが増加することを示す。
  このグループで実行されるプロセスに追加されるマイクロ秒数です。 この数値は
  スワッピングやメタデータのIOが大量に発生すると、この数値はかなり大きくなる可能性があるため、以下のように制限している。
  個々の遅延イベントを一度に1秒に制限している。

犠牲となったグループが再びレイテンシターゲットを満たし始めると、次のようになります。
を開始します。 犠牲となった
グループがIOを停止すると、グローバルカウンタは適切にスロットルを解除します。

IOレイテンシインターフェースファイル
~~~~~~~~~~~~~~~~~~~~~~~~~~

  io.latency
	これは他のコントローラと同様の形式をとる。

		"MAJOR:MINOR target=<マイクロ秒単位のターゲット時間>"

  io.stat
	コントローラが有効になっている場合、io.statに通常の統計に加えて、追加の統計が表示されます。
	に通常の統計に加えて

	depth
		グループの現在のキューの深さ。

 avg_lat
		これは指数移動平均で、減衰率は1/expである。
		である指数移動平均である。 減衰率の間隔は
		io.statの勝利値に対応するサンプル数を乗じることで計算できる。
		に対応するサンプル数を乗算することで計算できる。

	  win
		ミリ秒単位のサンプリングウィンドウサイズ。 これは評価イベント間の最小
		評価イベント間の最小継続時間。 ウィンドウが経過するのは
		のみ経過する。 アイドル期間は直近のウィンドウを延長する。

IO優先度
~~~~~~~~~~~

1つの属性がI/Oプライオリティcgroupポリシーの動作を制御します、
すなわち io.prio.class 属性です。この属性には次の値が使用できます。
その属性には

  no-change
	I/O優先クラスを変更しない。

  promote-to-rt
	RT以外のI/O優先度クラスを持つ要求に対して、それをRTに変更する。
	また、これらの要求の優先レベルを4に変更する。を変更しない。

  restrict-to-be

	優先度クラスRTを持つリクエストのI/O優先度を変更しないでください。  制限ツー
	I/O優先クラスを持たないか、I/O優先クラスRTを持つリクエストの場合
	優先度クラスRTを持つリクエストの場合、それをBEに変更する。また、これらのリクエストの優先レベルを0に変更する。
	これらのリクエストの優先レベルを0に変更します。
	優先度クラスIDLEを持つリクエストのI/O優先度クラスは変更しない。

  idle
	すべてのリクエストのI/O優先度クラスをIDLEに変更する。
	に変更する。


  none-to-rt
	非推奨。promote-to-rtの単なるエイリアス。

以下の数値は、I/O優先度ポリシーに関連付けられている:

+----------------+---+
| no-change      | 0 |
+----------------+---+
| promote-to-rt  | 1 |
+----------------+---+
| restrict-to-be | 2 |
+----------------+---+
| idle           | 3 |
+----------------+---+

各I/O優先度クラスに対応する数値は以下の通りである:

+-------------------------------+---+
| IOPRIO_CLASS_NONE             | 0 |
+-------------------------------+---+
| IOPRIO_CLASS_RT (real-time)   | 1 |
+-------------------------------+---+
| IOPRIO_CLASS_BE (best effort) | 2 |
+-------------------------------+---+
| IOPRIO_CLASS_IDLE             | 3 |
+-------------------------------+---+

リクエストのI/O優先クラスを設定するアルゴリズムは以下のとおりである：

- I/O優先クラスポリシーがpromote-to-rtの場合、リクエストのI/O優先クラス をIOPRIO_class_idleに変更する。
  リクエストのI/O優先クラスをIOPRIO_CLASS_RTに変更し、リクエストI/O優先レ ベルを4に変更する。
  レベルを4に変更する。
- I/O優先クラス・ポリシーがpromote-to-rtでない場合は、I/O優先クラス・ポリシーを数値に変換し、要求I/O優先レベルを4に変更する。
  クラス・ポリシーを数値に変換し、要求I/O優先度クラスをI/O優先度クラスの最大値
  をI/O優先クラス・ポリシー番号と数値I/O優先クラスの最大値に変更する。
  I/O優先クラス

PID
---

プロセス番号コントローラは、cgroup が新しいタスクの fork() や clone() を停止するために使用される。
を停止させるために使用される。
を停止するために使用されます。

cgroup 内のタスク数は、他のコントローラでは防げないような方法で枯渇する可能性があります。
コントローラを使用する必要があります。 たとえば
たとえば、フォーク爆弾は、メモリ制限に達する前にタスク数を使い果たす可能性が高い。
を使い果たす可能性が高い。

このコントローラで使用されるPIDは、カーネルで使用されるTID、プロセスIDを指すことに注意してください。
を指すことに注意してください。

PIDインターフェースファイル
~~~~~~~~~~~~~~~~~~~

  pids.max
	非ルートグループに存在する読み書き可能な単一値ファイル。
	cグループに存在する読み書き可能な単一値ファイル。 デフォルトは "max" です。

	プロセス数のハードリミット。

  pids.current
	すべての cgroup に存在する読み取り専用の単一値ファイル。

	すべての cgroup に存在する読み取り専用の単一値ファイル。
	の子孫にあるプロセスの数。

組織操作は cgroup ポリシーによってブロックされないため、pids.current > pids とすることができます。
pids.current > pids.max にすることができます。 これには次の方法があります。
制限をpids.currentより小さく設定するか、pids.currentがpids.maxより大きくなるように
プロセスをcgroupにアタッチして、pids.currentが
pids.maxより大きくなるようにcgroupに十分なプロセスをアタッチする。 ただし、fork() や clone() を使用して cgroup PID ポリシーに違反することはできません。
に違反することはできません。これらは、新しいプロセスの作成によって
を返します。


Cpuset
------

cpuset "コントローラは、CPUとメモリノードの配置を制限するメカニズムを提供します。
cpuset "コントローラは、タスクのCPUとメモリノードの配置を
cpuset "コントローラは、タスクのCPUとメモリノードの配置を、 タスクの現在のcgroupのcpusetインターフェースファイルで指定されたリソースだけに 制限するメカニズムを提供します。
これは特に大規模なNUMAシステムで有用です。
システムの適切なサイズのサブセットにジョブを配置する場合、これは特に有用です。
を慎重に配置することで、システム全体のパフォーマンスを向上させることができます。
にジョブを配置することで、システム全体のパフォーマンスを向上させることができます。

cpuset」コントローラは階層型です。 つまり、コントローラは
は、その親で許可されていないCPUやメモリノードを使用することはできません。


cpusetインターフェースファイル
~~~~~~~~~~~~~~~~~~~~~~

  cpuset.cpus
	非ルート上に存在する読み書き可能な複数値ファイル。
	cpuset-enabled cgroups に存在する読み書き可能な複数値ファイルです。
	これは、この
	をリストアップする。 しかし、実際に割り当てられる CPU のリストは
	しかし、実際に付与される CPU のリストは、その親によって課される制約に従う。
	要求された CPU とは異なることがある。

	CPU番号はカンマ区切りの数値または範囲である。
	たとえば次のようになる::

	  # cat cpuset.cpus
	  0-4,6,8-10

	空の値は、cgroupが最も近いcgroupの先祖と同じ設定を使用していることを示します。
	設定を使用していることを示します。
	"cpuset.cpus "がない場合は、使用可能なすべてのCPUを使用します。

	cpuset.cpus "の値は、次の更新まで一定である。
	CPUホットプラグイベントの影響を受けません。

  cpuset.cpus.effective
	すべての
	ファイルです。

	このファイルは、その親によってこの
	を一覧表示します。 これらの CPU は
	タスクによって使用される。

	"cpuset.cpus "が空の場合、"cpuset.cpus.effective "ファイルには、親cgroupからのすべてのCPUが表示される。
	を表示する。
	を表示します。 それ以外の場合は
	「のサブセットでなければなりません。
	のサブセットでなければならない。 この場合、空の
	空の "cpuset.cpus "と同様に扱われる。

	この値は、CPUホットプラグイベントの影響を受ける。

  cpuset.mems
	非ルートに存在する読み書き可能な複数値ファイル。
	cpuset が有効な cgroup に存在する読み書き可能な複数の値ファイル。

	cgroup内のタスクが使用するメモリ・ノードをリストする。
	この cgroup 内のタスクが使用する要求メモリノードをリストします。 しかし、実際に許可されるメモリノードのリスト
	は親によって課される制約に従うため、要求されたメモリノードとは異なることがあります。
	要求されたメモリノードとは異なることがあります。

	メモリノード番号は、カンマで区切られた番号または範囲です。
	例えば、以下のようになる::

	  # cat cpuset.mems
	  0-1,3

	空の値は、cgroup が空でない最も近い cgroup の祖先と同じ設定を使用していることを示します。
	と同じ設定を使用していることを示します。
	"cpuset.mems"、または使用可能なメモリノードがない場合はすべて
	と同じ設定を使用していることを示します。

	cpuset.mems」の値は、次の更新まで一定で
	cpuset.mems "の値は次の更新まで一定で、メモリノードのホットプラグイベントの影響を受けません。

	cpuset.mems" に空でない値を設定すると、cgroup 内のタスクのメモリが移行されます。
	cpuset.mems "に空でない値を設定すると、cgroup内のタスクのメモリが指定ノードに移行されます。
	このメモリ移行にはコストがかかります。

	このメモリ移行にはコストがかかります。 移行
	が完全でない場合があり、一部のメモリー・ページが残されることがある。
	そのため、新しいタスクを指定ノードに起動する前に、"cpuset.mems "を適切に設定することを推奨します。
	を適切に設定することを推奨します。 たとえ
	cpuset.mems "を変更する必要があるとしても、それを頻繁に行うべきではない。
	を頻繁に変更すべきではない。

  cpuset.mems.effective
	すべての
	ファイル。

	このファイルには、親からこの cgroup に実際に付与されているオンラインメモリノードが一覧表示されます。
	オンラインメモリノードを一覧表示します。これらのメモリノードは
	現在の cgroup 内のタスクによって使用される。

	cpuset.mems "が空の場合、このcgroupで使用可能な、親cgroupのすべてのメモリノードが表示される。
	のすべてのメモリノードを表示します。
	それ以外の場合は、"cpuset.mems" のサブセットである必要があります。
	のサブセットでなければなりません。 この場合
	この場合、空の "cpuset.mems "と同じように扱われる。

	この値は、メモリノードのホットプラグイベントの影響を受けます。

  cpuset.cpus.exclusive
	非ルートに存在する読み書き可能な複数値ファイル。
	cpusetが有効なcgroupに存在する読み書き可能な複数の値ファイル。

	これは、新しいcpusetを作成するために
	を一覧表示します。 この値は
	cgroup が有効なパーティションルートにならない限り使用されません。 この値は
	「cpuset.cpus.partition」セクションを参照してください。
	を参照してください。

	cgroup がパーティションルートになると、そのパーティションに割り当てられる実際の排他的な
	パーティションに割り当てられているCPUは
	「cpuset.cpus.exclusive.effective" にリストされます。
	これは "cpuset.cpus.exclusive "とは異なる可能性があります。 もし "cpuset.cpus.exclusive"
	が設定されている場合、"cpuset.cpus.exclusive.effective" は常にそのサブセットである。
	は常にそのサブセットです。

	ユーザーは手動で、"cpuset.cpus.exclusive.effective "とは異なる値に設定することができる。
	「cpuset.cpus "とは異なる値に手動で設定できる。	設定における唯一の制約は
	CPUのリストは、兄弟に対して排他的でなければならない

	ルートcgroupはパーティションルートで、利用可能なCPUはすべてその専用CPUセット
	はその排他的CPUセットにあります。

  cpuset.cpus.exclusive.effective
	すべての非ルートcpuset有効cgroupに存在する読み取り専用の複数値ファイル。
	cpuset が有効なすべての非 root cgroups に存在する読み取り専用の複数値ファイルです。

	このファイルは、パーティション
	の有効なセットを示します。 この
	ファイルの内容は、常に "cpuset.cpus "のサブセットとなり、その親の
	"cpuset.cpus.exclusive.effective "のサブセットになります。
	cgroupのサブセットになります。 また、"cpuset.cpus.exclusive "が設定されている場合は、そのサブセットになる。
	のサブセットにもなる。 cpuset.cpus.exclusive "が設定されていない場合は、暗黙の値として
	cpuset.cpus.exclusive "が設定されていない場合は、ローカルパーティション
	という暗黙の値を持つものとして扱われます。

  cpuset.cpus.isolated
	読み取り専用でルートcgroupのみの複数値ファイル。

	このファイルは、既存の分離されたパーティションで使用されるすべての分離されたCPUのセットを示します。
	を表示します。孤立パーティション
	が作成されていない場合は空になります。

  cpuset.cpus.partition
	非ルートの
	cpuset が有効な cgroup に存在する読み書き可能な単一値ファイルです。 このフラグは親cグループ
	によって所有され、委譲できません。

	書き込まれた場合、以下の入力値のみを受け入れます。

	  ========== =====================================
	  "member" パーティションの非ルートメンバー
	  「root" パーティション・ルート
	  "isolated" 負荷分散を行わないパーティション・ルート
	  ========== =====================================

	cpusetパーティションは、cpusetが有効なcグループの集合体です。
	階層の最上位にあるパーティション・ルートとその子孫である
	パーティションルート自身とその子孫を除きます。
	その子孫を除きます。 パーティションは
	に排他的にアクセスできます。	そのパーティション以外の
	パーティションの外側にある他のcgroupは、そのセットのCPUを使用できません。

	パーティションにはローカルとリモートの2種類があります。 ローカル
	パーティションとは、親cグループが有効なパーティションルートであるパーティションです。
	ルートです。 リモートパーティションは、親cgroupが有効なパーティションルートではないパーティションです。
	親cgroupが有効なパーティションルートではないものです。 "cpuset.cpus.exclusive "への書き込みは任意です。
	への書き込みは、ローカル・パーティションの作成ではオプションです。
	"cpuset.cpus.exclusive "ファイルに書き込むことは、ローカルパーティションの作成では任意です。
	を暗黙の値とします。	適切な
	適切な "cpuset.cpus.exclusive "値をcgroup階層に書き込むことは必須です。
	リモートパーティションの作成には、ターゲットパーティションルートの前に
	リモートパーティションの作成には必須です。

	現在、リモートパーティションはローカルパーティションの下に作成できません。
	パーティションの下に作成することはできません。 現在のところ、リモートパーティションをローカルパーティションの下に作成することはできません。
	を除くすべての先祖はパーティションルートになることはできません。

	ルートcgroupは常にパーティションルートであり、その状態を変更することはできません。
	状態を変更することはできません。 root 以外の cgroup はすべて "member" として開始します。

	root "に設定すると、現在のcgroupが新しいパーティションまたはスケジューリングドメインのルートになります。
	パーティションまたはスケジューリングドメインのルートになります。 排他CPUのセットは
	cpuset.cpus.exclusive.effective" の値によって決定される。

	isolated "に設定すると、そのパーティションのCPUは、スケジューラからの負荷分散を受けずに
	スケジューラからのロードバランシングを受けずに隔離された状態になります。
	になり、非バインド・ワークキューから除外されます。 このような複数のCPUを持つ
	このようなパーティションに配置されたタスクは、各CPUに慎重に分散され、バインドされる必要があります。
	このような複数のCPUを持つパーティションに配置されたタスクは、最適なパフォーマンスを得るために、各CPUに注意深く分散され、バインドされなければなりません。

	パーティション・ルート（"root "または "isolated"）には、有効または無効の2つの状態があります。
	パーティション・ルート（"root "または "isolated"）には、有効または無効の2つの状態があります。 無効なパーティション
	ルートはデグレードされた状態にあり、一部の状態情報は保持される可能性があります。
	のような振る舞いをします。

	member"、"root"、"isolated "の間で可能なすべての状態遷移が許される。
	"isolated "の間で可能なすべての状態遷移が許可される。

	読み取り時に、"cpuset.cpus.partition "ファイルは以下の値を示すことができる。
	の値が表示されます。

	  ============================= =====================================
	  "member" パーティションの非ルートメンバー
	  「root" パーティション・ルート
	  "isolated "パーティション・ルート（負荷分散なし
	  "root invalid (<理由>)"	無効なパーティションルート
	  "孤立無効（<理由>）"	孤立したパーティションルートが無効
	  ============================= =====================================

	無効なパーティション・ルートの場合、パーティションが無効である理由を説明する文字列が表示されます。
	パーティション・ルートが無効な場合、なぜ無効なのかを示す説明文字列が括弧内に含まれます。

	ローカル・パーティション・ルートが有効であるためには、次の条件を満たす必要があります。
	を満たす必要があります。

	1) 親cgroupが有効なパーティションルートである。
	2) "cpuset.cpus.exclusive.effective "ファイルが空であってはならない、
	   ただし、オフラインのCPUが含まれている可能性がある。
	3) 「cpuset.cpus.exective」は、このパーティションに関連するタスクがない限り、空にはできない。
	   cpuset.cpus.effective" を空にすることはできません。

	リモートパーティションルートが有効であるためには、最初の条件を除いて、上記のすべての条件を満たす必要があります。
	を除くすべての条件を満たす必要があります。

	ホットプラグのような外部イベントや、"cpuset.cpus "や
	「パーティションルートが無効になることがあります。
	が無効になることがあります。	タスクは
	タスクは "cpuset.cpus.effective" が空の cgroup には移動できないことに注意してください。

	有効な非ルートの親パーティションは、子ローカルパーティションにCPUを分配することができます。
	有効な非ルート親パーティションは、タスクが関連付けられていない場合
	有効な非root親パーティションは、タスクが関連付けられていない場合

	有効なパーティションルートを "member "に変更するには注意が必要です。
	パーティション・ルートを "メンバー "に変更する際には注意が必要です。
	子ローカルパーティションが存在する場合、その子ローカルパーティションはすべて無効となり
	子パーティションで実行されているタスクに支障をきたします。これらの無効化されたパーティションは
	親パーティションをパーティションルートに戻し、"cpuset.update "の値を適切なものにすれば、回復できます。
	パーティション・ルートに戻せば回復する可能性があります。

	pollイベントとinotifyイベントは、"cpuset.cpus.exclusive "の状態が変化するたびに発生します。
	"cpuset.cpus.partition "の状態が変更されるたびに発生します。 これには
	これは、"cpuset.cpus.partition "への書き込み、cpuのホットプラグ、またはその他の変更によって引き起こされる変更を含みます。
	パーティションの有効状態を変更するような変更も含まれます。
	これにより、ユーザースペースエージェントは、"cpuset.cpus.partition "への予期せぬ変更
	を監視することができるようになります。
	ポーリングする必要がなくなります。

	ユーザーは、"isolcpus "を使って、起動時にロードバランシングを無効にして、特定のCPUをあらかじめ隔離状態に設定することができる。
	に設定することができます。
	カーネルブートコマンドラインオプションを使用する。 これらのCPUを
	パーティションに入れる場合は、分離パーティションで使用する必要があります。


デバイスコントローラ
-----------------

デバイスコントローラはデバイスファイルへのアクセスを管理します。これには
新しいデバイスファイルの作成（mknodを使用）と、既存のデバイスファイルへのアクセスを含みます。
既存のデバイスファイルへのアクセスの両方が含まれます。

Cgroup v2 デバイスコントローラにはインターフェースファイルはなく、cgroup BPF 上で実装されています。
cgroup BPF 上に実装されています。デバイスファイルへのアクセスを制御するには、ユーザーは次のようにします。
BPF_PROG_TYPE_CGROUP_DEVICE タイプの bpf プログラムを作成し、BPF_PROG_TYPE_CGROUP_DEVICE で cgroup にアタッチします。
それらを BPF_CGROUP_DEVICE フラグを持つ cgroup にアタッチします。デバイスファイルにアクセスしようとすると
デバイスファイルにアクセスしようとすると、対応するBPFプログラムが実行されます。
戻り値によって、その試みは -EPERM で成功または失敗します。

BPF_PROG_TYPE_CGROUP_DEVICE プログラムは
bpf_cgroup_dev_ctx 構造体へのポインタを取ります：
アクセス・タイプ（mknod/read/write）とデバイス（タイプ、メジャー番号、マイナー番号）。
プログラムが0を返した場合、試みは-EPERMで失敗し、そうでなければ成功する。
で成功する。

BPF_PROG_TYPE_CGROUP_DEVICE プログラムの例は
tools/testing/selftests/bpf/progs/dev_cgroup.c にある。


RDMA
----

rdma "コントローラは、RDMAリソースの分配とアカウンティングを制御する。
を制御する。

RDMAインターフェースファイル
~~~~~~~~~~~~~~~~~~~~

  rdma.max
	rootを除くすべてのcgroupに存在する、読み書き可能なネストキー付きファイル。
	rootを除くすべてのcgroupに存在し、RDMA/IBデバイスに現在設定されているリソース制限を記述する
	を記述する。

	行はデバイス名でキーが付けられており、順序は付けられていません。
	各行には、スペースで区切られたリソース名と、設定され
	制限を含んでいる。

	以下のネストしたキーが定義されている。

	  ========== =============================
	  hca_handle HCAハンドルの最大数
	  hca_object HCA オブジェクトの最大数
	  ========== =============================

	mlx4 と ocrdma デバイスの例を以下に示す::

	  mlx4_0 hca_handle=2 hca_object=2000
	  ocrdma1 hca_handle=3 hca_object=max

  rdma.current
	現在のリソース使用量を記述する読み取り専用ファイル。
	root 以外のすべての cgroup に存在する。

	mlx4とocrdmaデバイスの例を以下に示す::

	  mlx4_0 hca_handle=1 hca_object=20
	  ocrdma1 hca_handle=1 hca_object=23

HugeTLB
-------

HugeTLBコントローラは、コントロールグループごとにHugeTLBの使用量を制限することができ
ページフォルト時にコントローラの制限を強制する。

HugeTLBインターフェースファイル
~~~~~~~~~~~~~~~~~~~~~~~


  hugetlb.<hugepagesize>.current
	hugepagesize "hugetlbの現在の使用量を表示する。 rootを除く全ての
	に存在する。

  hugetlb.<hugepagesize>.max
	hugepagesize "hugetlbの使用量の上限を設定/表示する。
	デフォルト値は "max "である。 root以外の全てのcgroupに存在する。

  hugetlb.<hugepagesize>.events
	root以外のcgroupに存在する読み取り専用のフラットキーファイル。

	  最大
		HugeTLB制限による割り当て失敗数

  hugetlb.<hugepagesize>.events.local
	hugetlb.<hugepagesize>.eventsに似ているが、ファイル内のフィールドは
	のフィールドはcgroupにローカルである。ファイル変更イベント
	はローカルイベントのみを反映します。

  hugetlb.<hugepagesize>.numa_stat
	memory.numa_stat と同様、<hugetlb.<hugepagesize> の hugetlb ページの numa 情報を表示する。
        の numa 情報を表示する。 アクティブな
        使用されている hugetlb ページだけが含まれます。 ノードごとの値はバイト単位である。
