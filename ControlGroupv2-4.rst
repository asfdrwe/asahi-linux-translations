`Control Group v2 <https://docs.kernel.org/admin-guide/cgroup-v2.html>`_  の非公式日本語訳です。
ライセンスは原文のライセンス(GPL-2.0のはず)に従います。

訳注: 

* まだDeepLの結果をはる途中(2024/3/10)
* ファイルがでかすぎるようなのでファイル分割(2024/3/10)

その他
----

Miscellaneous cgroup は、他の cgroup のように抽象化できないスカラーリソースのリソー ス制限とトラッキングメカニズムを提供する。
他の cgroup リソースのように抽象化できないスカラーリソースの
cgroup リソースのように抽象化できません。コントローラーは CONFIG_CGROUP_MISC コンフィグオプションで有効になります。
オプションによって有効になります。

の enum misc_res_type{} でコントローラにリソースを追加できます。
インクルード/linux/misc_cgroup.hファイル内のenum misc_res_type{}で、対応する名前を
で対応する名前を指定します。リソースの提供者は、リソースを使用する前に
リソースの提供者は、リソースを使用する前に misc_cg_set_capacity() を呼び出して容量を設定する必要があります。

容量が設定されると、charge および uncharge API を使用してリソースの使用量を更新できます。
unchargeAPIを使用して更新できます。miscコントローラとやりとりするためのAPIはすべて
にあります。

Miscインターフェースファイル
~~~~~~~~~~~~~~~~~~~~

Miscコントローラーは3つのインターフェースファイルを提供する。2つのmiscリソース(res_aとres_b)が登録されている場合：

  misc.capacity
        ルート cgroup にのみ表示される読み取り専用のフラットキーファイルです。 これは
        プラットフォームで利用可能な雑多なスカラーリソースとその数量を示す。
        とともに表示される::

	  $ cat misc.capacity
	  res_a 50
	  res_b 10

  misc.current
        すべてのcグループに表示される読み取り専用のフラットキーファイル。 これは
        cgroupとその子グループのリソースの現在の使用状況が表示されます::

	  $ cat misc.current
	  res_a 3
	  res_b 0

  misc.max
        非ルートの cgroup に表示される読み書き可能なフラットキーファイル。許可される
        cgroupとその子グループのリソースの最大使用量::

	  $ cat misc.max
	  res_a max
	  res_b 4

	上限は次のようにして設定する::

	  # echo res_a 1 > misc.max

	リミットを最大に設定するには次のようにする::

	  # echo res_a max > misc.max

        リミットは misc.capacity
        ファイルの容量値より高く設定できる。

  misc.events
	非ルートcグループに存在する読み取り専用のフラットキーファイル。以下の項目が定義されている。
	以下のエントリが定義されている。特に指定がない限り
	を変更すると、ファイル変更イベントが生成されます。このファイルのすべてのフィールド
	フィールドはすべて階層化されています。

	  max
		cgroup のリソース使用量が最大値を超えそうになった回数。
		を超えた回数。

マイグレーションとオーナーシップ
~~~~~~~~~~~~~~~~~~~~~~~

雑多なスカラーリソースは、そのリソースが最初に使用された cgroup にチャージされます。
にチャージされ、そのリソースが解放されるまでその cgroup にチャージされたままになります。プロセスを
プロセスが別のcgroupに移動しても、チャージは移動先のcgroupには移動しません。
cgroup には移動しません。

その他
------

perf_event
~~~~~~~~~~

perf_event コントローラは、レガシー階層にマウントされていない場合、v2 階層で自動的に有効になります。
v2階層で自動的に有効になる。
常に cgroup v2 パスでフィルタリングされる。 コントローラは
コントローラは、v2階層が設定された後でもレガシー階層に移動できます。


非基準情報
-------------------------

このセクションには、安定版カーネルAPIの一部とはみなされない情報が含まれています。
そのため、変更される可能性があります。

CPU コントローラールート cgroup プロセスの動作
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ルートcgroupでCPUサイクルを分配する場合、このcgroupの各スレッドは
cgroup内の各スレッドは、ルートcgroupの別の子cgroupでホストされているかのように扱われます。
でホストされているかのように扱われます。この子 cgroup のウェイトは、そのスレッドナイス
レベルに依存します。

このマッピングの詳細については
を参照のこと（この配列の値は適切にスケーリングされる必要がある）。この配列の値は適切にスケーリングされる必要があるため、中立値 - nice 0 - は 1024 ではなく 100 になる)。


IO コントローラールート cgroup プロセスの動作
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ルートcgroupでCPUサイクルを分配する場合、このcgroupの各スレッドは
cgroup内の各スレッドは、ルートcgroupの別の子cgroupでホストされているかのように扱われます。
でホストされているかのように扱われます。この子 cgroup のウェイトは、そのスレッドナイス
レベルに依存します。

このマッピングの詳細については
を参照のこと（この配列の値は適切にスケーリングされる必要がある）。この配列の値は適切にスケーリングされる必要があるため、中立値 - nice 0 - は 1024 ではなく 100 になる)。


IO コントローラールート cgroup プロセスの動作



ルート cgroup プロセスは暗黙のリーフチャイルドノードでホストされます。IOリソースを分配するとき、この暗黙の子ノードは通常の子ノードと同じように考慮されます。
を持つルート cgroup の通常の子 cgroup であるかのように考慮されます。重み値 200 を持つルート cgroup の通常の子 cgroup のように考慮されます。


名前空間
=========

基本
------

cgroup 名前空間は、"/proc/$PID/cgroup" ファイルと cgroup マウントのビューを仮想化するメカニズムを提供します。
「ファイルおよび cgroup マウントを仮想化するメカニズムを提供します。 CLONE_NEWCGROUP クローン
フラグを clone(2) および unshare(2) で使用すると、新しい cgroup名前空間を作成します。 cgroup 名前空間の内部で実行されているプロセスは
その "/proc/$PID/cgroup" 出力は cgroupns root に制限されます。 この
cgroupns root は、cgroup 名前空間の作成時のプロセスの cgroup です。

cgroup 名前空間がない場合、"/proc/$PID/cgroup" ファイルはプロセスの cgroup の完全なパスを示す。
ファイルには、プロセスの cgroup の完全なパスが表示される。 コンテナのセットアップでは
コンテナのセットアップでは、cgroup とネームスペースのセットがプロセスの分離を目的としているため、"/proc/$PID/cgroup" ファイルはプロセスの cgroup の完全なパスを示す。
"/proc/$PID/cgroup "ファイルは、隔離されたプロセスにシステムレベルの情報を漏らす可能性がある。
を漏洩する可能性がある。 たとえば、以下のようになる::

  # cat /proc/self/cgroup
  0::/batchjobs/container_id1

このパス「/batchjobs/container_id1」は、システム・データと見なせる。
であり、孤立プロセスに公開するのは望ましくない。を使用して、このパスの可視性を制限することができる。 例えばcgroup名前空間を作成する前は、次のように表示される::

  # ls -l /proc/self/ns/cgroup
  lrwxrwxrwx 1 root root 0 2014-07-15 10:37 /proc/self/ns/cgroup -> cgroup:[4026531835]
  # cat /proc/self/cgroup
  0::/batchjobs/container_id1

新しいネームスペースの共有を解除すると、以下のように表示が変わります::

  # ls -l /proc/self/ns/cgroup
  lrwxrwxrwx 1 root root 0 2014-07-15 10:35 /proc/self/ns/cgroup -> cgroup:[4026532183]
  # cat /proc/self/cgroup
  0::/

マルチスレッドプロセスの一部のスレッドがその cgroup名前空間の共有が解除されると、新しい cgroupns がプロセス全体（すべてのスレッド
スレッド) に適用されます。 これは v2 階層では自然ですがレガシー階層では予期しないことがあります。cgroup 名前空間は、
その内部にプロセスがあるかマウントがピン留めしている限り存続します。最後の使用者がいなくなると、cgroupネームスペースは破棄されます。
cgroupns ルートと実際の cgroupsは残ります。

ルートとビュー
------------

cgroup 名前空間の 'cgroupns root' は、unshare(2) を呼び出したプロセスが実行されている cgroup である。
である。 たとえば
/batchjobs/container_id1 cgroup 内のプロセスが unshare を呼び出す場合、 cgroup
/batchjobs/container_id1 が cgroupns ルートになります。 そのため
init_cgroup_ns では、これが本当のルート ('/') cgroup になります。

ネームスペース作成プロセスが後で別の cgroup に移動しても、cgroupns ルート cgroup は変更されません。
プロセスが後で別のcgroupに移動しても、cgroupnsルートcgroupは変更されません：

  # ~/unshare -c # あるcgroup内のcgroupnsの共有を解除する。
  # cat /proc/self/cgroup
  0::/
  # mkdir sub_cgrp_1
  # echo 0 > sub_cgrp_1/cgroup.procs
  # cat /proc/self/cgroup
  0::/sub_cgrp_1

各プロセスは、"/proc/$PID/cgroup" のネームスペース固有のビューを取得する。

cgroup 名前空間の内部で実行されているプロセスには、以下のものが表示されます。
cgroup パス（/proc/self/cgroup 内）を見ることができます。
非共有 cgroupns 内から:：

  # sleep 100000 &
  [1] 7353
  # echo 7353 > sub_cgrp_1/cgroup.procs
  # cat /proc/7353/cgroup
  0::/sub_cgrp_1

最初のcgroupネームスペースから、実際のcgroupパスは次のようになる。
になります：

  $ cat /proc/7353/cgroup
  0::/batchjobs/container_id1/sub_cgrp_1

兄弟 cgroup ネームスペース（つまり、別の cgroup をルートとするネームスペース）からは
をルートとするネームスペース) からは、自身の cgroup
ネームスペースのルートからの相対 cgroup パスが表示されます。 例えば、PID 7353 の cgroup
ネームスペースのルートが '/batchjobs/container_id2' にある場合、次のように表示される：

  # cat /proc/7353/cgroup
  0::/../container_id2/sub_cgrp_1

相対パスは常に「/」で始まることに注意してください。
で始まることに注意してください。


移行と setns(2)
----------------------

cgroup 名前空間内のプロセスは、適切な権限があれば
ネームスペースルートに移動できます。 例えば
に cgroupns ルートを持つネームスペースの内部から、以下のように移動できます。
/バッチジョブズ/コンテナ_id1 をルートとするネームスペースの内部から、グローバル階層がcgroupns::の内部でまだアクセス可能であると仮定します。
cgroupns:内でアクセス可能であると仮定します：

  # cat /proc/7353/cgroup
  0::/sub_cgrp_1
  # echo 7353 > batchjobs/container_id2/cgroup.procs
  # cat /proc/7353/cgroup
  0::/../container_id2

このような設定は推奨されないことに注意。 cgroup
名前空間内のタスクは、その cgroupns 階層にのみ公開されるべきです。

別の cgroup 名前空間への setns(2) は、以下の場合に許可される：

(a) プロセスが現在のユーザ名前空間に対して CAP_SYS_ADMIN を持っている。
(b) ターゲット cgroup 名前空間のユーザ ns に対して、プロセスが CAP_SYS_ADMIN を持っている。
    ネームスペースのユーザ名に対して CAP_SYS_ADMIN を持っている。

別の cgroup ネームスペースにアタッチしても、暗黙の cgroup の変更は発生しません。
ネームスペースにアタッチしても、暗黙の cgroup の変更は発生しません。 誰かがアタッチプロセスをターゲットの
プロセスをターゲット cgroup 名前空間ルートの下に移動することが期待されます。


他のネームスペースとの相互作用
---------------------------------

ネームスペース固有の cgroup 階層は、以下のプロセスによってマウントできます。
非イット cgroup 名前空間内で実行しているプロセスによってマウントできます：

  # マウント -t cgroup2 none $MOUNT_POINT

これは、cgroupns ルートをファイルシステムルートとして統一 cgroup 階層をマウントします。
ファイルシステムルートとしてマウントします。 このプロセスには、ユーザーおよびマウントネームスペースに対して CAP_SYS_ADMIN が必要です。
マウントネームスペースに対して CAP_SYS_ADMIN が必要です。

このように /proc/self/cgroup ファイルの仮想化と
名前空間プライベートの cgroupfs マウントによる cgroup 階層のビューの制限と組み合わせた /proc/self/cgroup ファイルの仮想化は
を使用することで、コンテナ内で適切に分離された cgroup ビューが提供されます。


カーネルプログラミングに関する情報
=================================

このセクションでは、cgroup との対話が必要な領域でのカーネルプログラミング情報を記載します。
cgroup コアとコントローラは対象外です。
コントローラは対象外です。


ライトバックのファイルシステムサポート
--------------------------------

ファイルシステムは以下を更新することで cgroup ライトバックをサポートできます。
address_space_operations->writepage[s]() を更新することで、cgroup ライトバックをサポートできます。
を更新することでcgroup writebackをサポートできる。

  wbc_init_bio(@wbc, @bio)
	ライトバックデータを持つ各バイオに対して呼び出される。
	バイオをinodeの所有者cgroupと対応するリクエストキューに関連付ける。
	対応するリクエストキューに関連付ける。 これは
	キュー(デバイス)がバイオに関連付けられ
	提出の前に呼ばれなければならない。

  wbc_account_cgroup_owner(@wbc, @page, @bytes)
	書き出される各データセグメントに対して呼ばれるべきである。
	この関数は、ライトバックセッション中
	この関数は、ライトバックセッション中にいつ呼ばれるかは気にしませんが、 データセグメントが書き出されるときに呼ぶのが最も簡単で自然です。
	バイオにデータセグメントが追加されるときに呼び出すのが最も簡単で自然です。

ライトバック・バイオのアノテーションでは、cgroup サポートは
s_iflagsにSB_I_CGROUPWBを設定することで、super_blockごとにcgroupサポートを有効にすることができる。 これにより
これにより、cgroup writeback サポートを選択的に無効にすることができる。
ジャーナルデータモードなど、特定のファイルシステム機能に互換性がない場合に役立つ。
などが互換性がない場合に役立つ。

wbc_init_bio() は、指定されたバイオをその cgroup にバインドします。 設定によっては
設定によっては、バイオは低い優先度で実行されるかもしれない。
ライトバック・セッションが共有リソース、例えばジャーナル
エントリーのような共有リソースを保持している場合、優先順位が逆転する可能性があります。 この問題に対する簡単な解決策は
はありません。 ファイルシステムは、特定の問題ケースを回避するために
wbc_init_bio()をスキップして直接bio_associate_blkg()
を直接使用することで、特定の問題ケースを回避することができます。


非推奨 v1 コア機能
===========================

- 名前付き階層を含む複数の階層はサポートされません。

- すべての v1 マウントオプションはサポートされません。

- tasks" ファイルは削除され、"cgroup.procs" はソートされません。

- "cgroup.clone_children" は削除されました。

- /proc/cgroups は v2 では無意味です。 代わりに "cgroup.controllers "ファイル
  を使用する。


v1の問題点とv2の理由
====================================

複数の階層
--------------------

cgroup v1 では任意の数の階層を使用でき、各階層は任意の数のコントローラをホス トできました。
階層は任意の数のコントローラをホストできました。 これは
これは高い柔軟性を提供するように見えましたが、実際には役に立ちませんでした。

たとえば、各コントローラのインスタンスは1つだけであるため、freezerのようなユーティリティタイプのコントローラでは、各コントローラのインスタンスは1つだけである。
たとえば、各コントローラのインスタンスは1つしかないため、freezerのようなユーティリティタイプのコントローラは、すべての階層で使用できます。
などのユーティリティタイプのコントローラは、1つの階層でしか使用できません。 この問題は
この問題は、一度階層が作成されると、コントローラを別の階層に移動することができないという事実によって悪化します。
この問題は、一度階層が作成されると、コントローラを別の階層に移動させることができないという事実によって悪化しました。 もう一つの問題は
階層にバインドされたすべてのコントローラが、階層に対してまったく同じビューを持つことを強制されることです。
を強制されることです。 特定のコントローラによって粒度を変えることはできませんでした。
によって粒度を変えることができませんでした。

実際には、これらの問題は、どのコントローラを同じ階層に置くかを大きく制限しました。
ほとんどのコンフィギュレーションは、各コントローラをそれぞれの階層に置くことに頼っていました。
ほとんどのコンフィギュレーションは、各コントローラを独自の階層に置くことに頼っていた。 ほとんどの構成では、各コントローラを独自の階層に置くことに頼っていた。
cpuとcpuacctコントローラなど、密接に関連するものだけが、同じ階層に置く意味がありました。
階層に置くことに意味があった。 そのため、ユーザーランドが複数の同じような階層を管理することになり、同じことを繰り返すことになる。
を管理することになる。
を繰り返すことになる。

さらに、複数階層のサポートには大きなコストがかかった。
cgroupコアの実装が非常に複雑になったが、それ以上に重要なのは以下の点である。
複数の階層をサポートすることで、cgroupの一般的な使用方法やコントローラが制限されました。
の一般的な使用方法とコントローラにできることが制限されました。

階層の数に制限はなかった。
スレッドのcgroupメンバシップを有限の長さで記述できなかった。
つまり、スレッドの  キーは任意の数のエントリを含むことができ、長さは無制限だった。
キーは任意の数のエントリを含む可能性があり、長さは無制限だった。
メンバーシップを識別するためだけに存在するコントローラを追加することになった、
その結果、メンバーシップを識別するためだけに存在するコントローラーが追加されることになった。
を悪化させた。

また、コントローラは階層のトポロジーを予想することができなかった。
また、コントローラは、他のコントローラが存在する可能性のある階層のトポロジを予想することができないため、各コントローラは、他のすべてのコントローラを想定しなければならなかった。
各コントローラは、他のすべてのコントローラが完全に直交する階層に接続されていると仮定しなければならなかった。
各コントローラは、他のすべてのコントローラが完全に直交する階層に取り付けられていると仮定しなければならなかった。 そのため
このため、コントローラ同士が協力することは不可能であり、少なくとも非常に面倒であった。

ほとんどのユースケースでは、コントローラを互いに完全に直交する階層に置くことは
ほとんどのユースケースでは、互いに完全に直交する階層にコントローラを配置する必要はありません。 通常必要とされるのは
通常必要とされるのは、特定のコントローラに応じて、異なるレベルの粒度を持つことです。
です。 言い換えると、階層は
をリーフからルートに向かって折りたたむことができる。
つまり、特定のコントローラから見たときに、階層がリーフからルートに向かって折りたたまれることがあります。 たとえば、あるコンフィギュレーションでは
たとえば、あるコンフィギュレーションでは、メモリがあるレベル以 上にどのように分配されるかは気にしないが、CPUサイクルがどのように分配され るかは制御したい。
CPUサイクルがどのように分散されるかを制御したい。


スレッドの粒度
------------------

cgroup v1では、プロセスのスレッドを異なるcgroupに所属させることができました。
これはいくつかのコントローラにとっては意味がなく、そのようなコントローラは、結局、そのようなスレッドを無視するさまざまな方法を実装することになりました。
そのようなコントローラは、結局、そのような状況を無視する別の方法を実装することになった。
より重要なのは、個々のアプリケーションに公開されるAPIとシステム管理インターフェイスの境界を曖昧にしたことだ。
とシステム管理インターフェイスの境界が曖昧になる。

一般的に、プロセス内の知識はプロセス自身だけが利用できる。
したがって、プロセスのサービスレベル組織化とは異なり、プロセスのスレッドを分類することはできない、
プロセスのサービスレベル組織化とは異なり、プロセスのスレッドを分類するには、対象となるプロセスを所有するアプリケーションが積極的に参加する必要がある。
の積極的な参加が必要である。

cgroup v1 には曖昧に定義された委譲モデルがあり、スレッド粒度と組み合わせて悪用されました。
cgroupは個々のアプリケーションに委譲された。
cgroupは個々のアプリケーションに委譲され、アプリケーションは独自のサブ階層を作成して管理し、リソースを制御することができます。
サブヒエラルキーを作成して管理し、それに沿ったリソース配分を制御できるように、cgroups は個々のアプリケーションに委譲されました。 これにより
cgroupは事実上、素人プログラムに公開されるsyscallのようなAPIに昇格した。
のような状態に引き上げました。

まず第一に、cgroup はこのように公開するには根本的に不十分なインターフェースを持っています。
である。 プロセスが自分自身のノブにアクセスするには、以下の手順が必要です。
proc/self/cgroupからターゲット階層上のパスを抽出する、
ノブの名前をパスに追加してパスを構築し、ノブをオープンし、そのノブに対して読み取りや書き込みを行わなければならない。
を開き、そこに読み書きしなければならない。 これは非常に不便で珍しいだけでなく
なだけでなく、本質的にキビシイ。 従来の方法では
トランザクションを定義する従来の方法がない。
プロセスが実際に自分のサブ階層で動作していることを保証するものはない。

cgroupのコントローラは、パブリックAPIとしては決して受け入れられないノブを数多く実装していた。
を実装している。
cgroupは結局、適切に抽象化も洗練もされていないインターフェイスのノブ
インターフェイスのノブを実装することになりました。
カーネル内部の詳細を直接明らかにしてしまった。 これらのノブは
これらのノブは、定義されていない委譲メカニズムを通じて個々のアプリケーションに公開されます。
必要な精査を経ずにパブリック API を実装する近道として cgroup を悪用した。
を実装する近道として悪用された。

これはユーザーランドとカーネルの両方にとって痛手だった。 ユーザーランドは結局
カーネルは、不用意に構成要素を公開したり、ロックしたりすることになった。
カーネルは不注意に構成要素を公開し、ロックしてしまった。


内部ノードとスレッド間の競争
-------------------------------------------

cgroup v1 では、スレッドはどの cgroup にも属することができました。
親cgroupとその子cgroupに属するスレッドがリソースを奪い合うという興味深い問題が発生しました。
に属するスレッドがリソースを奪い合うという興味深い問題が発生しました。 これは
これは、2つの異なるタイプのエンティティが競合し、それを解決する明白な方法がなかったため、厄介でした。
解決する方法がなかった。 異なるコントローラーは異なることをした。

CPUコントローラーは、スレッドとcgroupを等価とみなし、いいレベルをcgroupの重みにマッピングした。
をcgroupのウェイトにマッピングした。 これはいくつかのケースでは機能したが
子どもたちが特定の比率のCPUサイクルを割り当てたいときに、内部スレッド数が
サイクルの特定の比率を割り当てられたかったり、内部のスレッド数が変動したりすると、この比率は平坦ではなかった。
競合するエンティティの数が変動すると、比率は常に変化した。
他にも問題があった。 ナイスレベルからウェイトへのマッピング
へのマッピングは明白で普遍的なものではなかった。
スレッドでは使用できなかった。

ioコントローラは暗黙のうちに、スレッドをホストするために各
を暗黙的に作成した。 隠しリーフには
leaf_``という接頭辞を持つ。 これにより
内部スレッドの制御は可能だが、重大な欠点があった。 それは
そうでなければ必要ない入れ子のレイヤーが常に追加される。
インターフェイスが乱雑になり、実装が著しく複雑になる。
実装を著しく複雑にしてしまう。

メモリー・コントローラーには、内部タスクと子cグループの間で起こることを制御する方法がなかった。
メモリーコントローラーには、内部タスクと子cgroupの間で何が起こるかを制御する方法がなく、動作は明確に定義されていなかった。
明確に定義されていなかった。 アドホックな振る舞いを追加する試みもあった。
特定のワークロードに合わせて動作を調整するために、アドホックな動作やノブを追加しようとする試みもあった。
長期的に解決するのが非常に難しい問題につながった。

複数のコントローラーが内部タスクと格闘し、さまざまな対処法を考え出した。
残念ながら、どのアプローチも重大な欠陥があった。
残念ながら、すべてのアプローチに重大な欠陥があり、さらに、大きく異なる動作によって
により、cgroup全体として非常に一貫性のないものになってしまった。

これは明らかに、統一された方法で cgroup コア
から統一された方法で対処する必要がある問題であることは明らかです。


その他のインターフェースの問題
----------------------

cgroup v1 は監視されることなく成長し、多くの特殊性と矛盾を生み出しました。
を開発しました。 cgroup コア側の問題の 1 つは
ユーザーランドのヘルパーバイナリがフォークされ、イベントごとに実行されました。
ユーザーランドのヘルパーバイナリがフォークされ、イベントごとに実行されました。 イベント配信は
再帰的または委譲可能ではなかった。 また、このメカニズムの限界から
カーネル内イベントデリバリー・フィルタリングメカニズムが、さらに複雑にしていた。
インターフェースをさらに複雑にしていた。

コントローラー・インターフェースにも問題があった。 極端な例は
コントローラーが階層構造を完全に無視し
極端な例としては、コントローラが階層構造を完全に無視し、すべてのcグループがルート
cgroupの直下にあるかのように扱うことである。 コントローラの中には、一貫性のない実装の詳細をユーザーランドに大量に公開するものがありました。
実装の詳細をユーザーランドに公開しているコントローラもありました。

また、コントローラ間の一貫性もありませんでした。 新しいcgroup
が作成されると、コントローラによってはデフォルトで余計な
を設定するまで、リソースの使用を許可しないものもありました。
を許可するものもありました。 同じタイプの
の設定ノブは、大きく異なる命名スキームとフォーマットを使用していました。 統計
と情報ノブは、同じコントローラであっても、任意に命名され、異なるフォーマットと単位を使用していた。
同じコントローラでも、フォーマットや単位が異なっていました。

cgroup v2は、必要に応じて共通の規約を確立し、コントローラを更新して、最小限の
コントローラを更新し、最小限の一貫したインターフェイスを公開します。


コントローラの問題と対策
------------------------------

メモリ
~~~~~~

本来の下限であるソフトリミットは、デフォルトで設定されていないリミットとして定義されている。
として定義されている。 その結果、global reclaim が優先する cgroup のセットは、opt-out ではなく opt-in になる。
のセットはオプトアウトではなくオプトインになります。 そのため
を最適化するためのコストは非常に高いため、実装は膨大なサイズにもかかわらず
実装は、その巨大なサイズにもかかわらず、基本的な望ましい動作さえ提供しない。
基本的な望ましい動作を提供しない。 まず第一に、ソフトリミットには
階層的な意味はない。 設定されたグループはすべてグローバルな
rbtreeに編成され、階層構造のどこに位置するかに関係なく、等しいピアのように扱われる。
に関係なく、対等なピアとして扱われる。 このため、サブツリーのデリゲーションは不可能である。 第二に
ソフト・リミットのリクレイム・パスは非常にアグレッシブであるため、システムに高いアロケーション・レイテンシーをもたらすだけでなく
システムに高いアロケーション・レイテンシーをもたらすだけでなく
システム性能に影響を与える。
は自滅する。

一方、memory.lowバウンダリーは、トップダウンで割り当てられる。
リザーブである。 cgroupはその有効なlowの範囲内であれば、再要求の保護を享受できる。
内にある場合、cgroup はリクレイムプロテクションを享受し、サブツリーのデリゲーションが可能になります。また
を超える場合は、オーバーエイジに比例したリクレイム圧力を享受できる。
を超える場合、そのオーバーエイジに比例したリクレイム圧力を享受する。

本来の高さの境界であるハードリミットは、次のように定義される。
たとえOOMキラーが呼ばれなければならないとしても、その限界は動かない。
しかし、これは一般的に、利用可能なメモリを最大限に活用するという目標に反する。
利用可能なメモリを最大限に活用するという目標に反する。 ワークロードのメモリ消費量は、実行中に変化する。
そのため、ユーザーはオーバーコミットする必要がある。 しかし
厳密な上限値でそれを行うには、作業セットのサイズをかなり正確に予測するか、作業セットのサイズにスラックを加える必要がある。
作業セットサイズをかなり正確に予測するか、上限値に余裕を持たせる必要がある。 作業セットサイズ
を予測するのは難しく、間違いが起こりやすい。
OOMが発生するため、ほとんどのユーザーは上限を緩くする側に回りがちで、結局貴重なリソースを無駄にしてしまう。
貴重なリソースを無駄にしてしまう。

一方、memory.highバウンダリは、より控えめに設定することができる。
より保守的に設定できる。 ヒットすると、割り当てを強制的に
しかし、OOMキラーを発動させることはない。
キラーを呼び出すことはない。 しかし、OOMキラーを起動することはない。
プロセスを終了させることはない。
徐々に性能が低下する。 ユーザーはこれを監視し
を監視し、許容可能な性能を維持できる最小限のメモリフットプリントが見つかるまで修正を加えることができる。
許容可能なパフォーマンスが得られる最小限のメモリフットプリントが見つかるまで、ユーザーはこれを監視し、修正を加えることができる。

極端な場合、多くの同時割り当てがあり
極端な場合、多くの同時割り当てがあり、グループ内の再要求の進行が完全に中断している場合、高い境界を超える可能性がある。
を超えることもある。 しかし、そのような場合でも、ほとんどの場合
しかし、そのような場合であっても、他のグループやシステムの残りの部分で利用可能なスラックから割り当てを満たす方が、グループを殺すよりも良い場合がほとんどです。
しかし、その場合でも、ほとんどの場合、グループを殺すよりも、他のグループやシステムの残りの部分で利用可能なスラックから割り当てを満たす方が良い。 そうでなければ、memory.maxは
メモリ.maxは、このようなスピルオーバーを制限し、最終的にバグのある、あるいは悪意のあるアプリケーションを封じ込めるためにある。
悪意のあるアプリケーションを封じ込める。

元のmemory.limit_in_bytesを現在の使用量以下に設定すると、競合状態が発生する。
を現在の使用量以下に設定することは、同時課金が制限設定を失敗させるというレースコンディションを引き起こす可能性があった。
一方、memory.maxは、新しいチャージを防ぐためにまずリミットを設定し、それから
を設定し、新しいリミットが達成されるまで再要求とOOM killを行う。
またはmemory.maxへの書き込みタスクが強制終了される。

メモリとスワップのアカウンティングとリミッターの組み合わせは、スワップ領域に対する実際のコントロールに置き換えられます。
スワップ領域の制御である。

オリジナルの
cgroup設計の主な議論は、グローバルまたは親のプレッシャーが常に
に関係なく、子グループのすべての匿名メモリをスワップできることでした。
に関係なく、子グループのすべての匿名メモリをスワップできることでした。 しかし、信頼されていない
グループは、他の手段でスワッピングを妨害することができる。
管理者は、信頼できないグループをオーバーコミットする際に、スワップ可能性を完全に想定することはできません。
管理者は、信頼されていないジョブをオーバーコミットする際に、スワップ可能性を完全に想定することはできません。

一方、信頼されたジョブの場合、複合カウンターは直感的なユーザーインターフェイスではありません。
直感的なユーザ空間インタフェースではなく、また
cgroupコントローラは特定の物理リソース
リソースを考慮し、制限すべきであるという考えに反しています。 スワップスペースはシステム内の他のリソースと同じです、
そのため、統一された階層構造によって、それを別々に分配することができるのです。
