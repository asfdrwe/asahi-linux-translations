[Beyond Gaming: X11 bridging in muvm](https://asahilinux.org/2024/12/muvm-x11-bridging/)の非公式日本語訳です。

訳注: 本家ブログへのリンクは対応する日本語訳へのリンクに変更

参考資料
- QEMU のような VM に関する資料: [GPU仮想化最前線 - KVMGTとvirtio-gpu -](https://www.slideshare.net/slideshow/gpu-kvmgtvirtiogpu/155641005)、ファイルシステム共有に関して [virtiofsについて](https://qiita.com/tmsn/items/120c6c2fb07cf32d2147)
- Waylandとに関する資料: [Waylandの構造](https://wayland.freedesktop.org/architecture.html)、[X WindowSystem プロトコルとアーキテクチャ](https://ja.wikipedia.org/wiki/X_Window_System%E3%83%97%E3%83%AD%E3%83%88%E3%82%B3%E3%83%AB%E3%81%A8%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3)、WaylandやXのクライアントとサーバが通信する仕組みの理解に必要な[ソケットやパイプとファイルディスクリプタの解説](https://zenn.dev/ganariya/articles/socket-slide-illustration-go-implement)
- 共有メモリと排他制御に関する資料: [メモリの仕組みと排他制御](https://zenn.dev/a3geek/articles/270d7946b80310)
- Futexに関する資料: [w_o’s diaryのfutex](https://w0.hatenablog.com/entry/20070323/p1)、[Ubuntuの日本語マニュアルでのFutex解説](https://manpages.ubuntu.com/manpages/focal/ja/man7/futex.7.html)、[futexシステムコール解説](https://manpages.ubuntu.com/manpages/focal/ja/man2/futex.2.html)、[Linuxのfutexの必要性について](https://togetter.com/li/41625)、

---
# ゲームを超えて: muvmによる X11 ブリッジ
- [前回](https://github.com/asfdrwe/asahi-linux-translations/blob/main/PROGRESS202410.md)

みなさん、こんにちは！Asahi Linuxのx86/x86-64エミュレーションスタックにとてもクールなアップデートがありましたので、
その内容をシェアしたいと思います。今日からゲーム以外のアプリでも使えるようになりました！

![cisco-pt](https://asahilinux.org/img/blog/2024/12/cisco-pt.png)

Fedora Asahi Remix上で動作するCisco Packet Tracer

## VM 上のネイティブグラフィックス

[以前のブログ記事](https://github.com/asfdrwe/asahi-linux-translations/blob/main/PROGRESS202410.md)を覚えているかもしれませんが、
Asahi Linux は [muvm](https://github.com/AsahiLinux/muvm) が駆動する microVM ですべての x86/x86-64 アプリケーションを実行しています。
ハードウェア GPU パススルーなしにネイティブに近い性能で実際の VM 上でゲームを実行するにはどうすればいいのでしょうか？

AMD/Intel　システムでは（そして実際、Apple Silicon 上の macOS でも）、GPU 仮想化は常にある種の制限を受けてきました。
選択肢は、ハードウェア GPU を完全にパススルーするか、API レベルの GPU 準仮想化(paravertualization)を使用するかです。ハードウェア GPU を
パススルーすることで GPU 機器を完全にゲストに割り当てます。つまり、『本物』の GPU 機器のように見えることを意味します。これはその GPU 用の
本物のドライバを持つすべてのゲスト OS上で動作し、ネイティブな性能になりますが、GPU　がゲスト専用であることを意味し、ホストと共有したり、ゲストと
ホストのウィンドウを1つの画面に統合したりすることはできません。一方、API レベルの GPU 準仮想化は、基本的に OpenGL や Vulkan や Metalコマンドを
ゲストからホストに送信し、ホストの GPU ドライバスタックによって処理されます。これはゲストに 『汎用(generic)』準仮想化 GPU ドライバを必要とし、
すべての高レベル GPU 描画コマンドがゲストからホストへの障壁を越え、ホスト上で処理されなければならないため、ネイティブ GPU の使用よりもはるかに遅くなります。
これが macOS での GPU 仮想化の仕組みです。一部の GPU(例えば最近の Nvidia GPU)はハードウェア仮想化サポートで真の共有に対応していますが、これは
まだ上流にはなく、ハードウェア対応が必要で、Apple GPU にはありません。

しかし、もっと良い方法があるとしたら？判明しました！[DRM Native Context](https://indico.freedesktop.org/event/2/contributions/53/attachments/76/121/XDC2022_%20virtgpu%20drm%20native%20context.pdf)です。

コンセプトはとてもシンプルです： ゲスト上で GPU ドライバスタック全体を実行しハードウェア GPU 全体を通過させる代わりに（共有なし）、もしくは、ホスト上で 
GPU ドライバスタック全体を実行して高レベル API を通過させる代わりに（遅い）、真ん中で分割するのはどうでしょうか？DRM Native Context は、
ホスト上で GPU *カーネルドライバ*を実行し、ゲスト上で GPU *ユーザースペースドライバ*（Mesa）を実行し、ゲストからホストへ*カーネル UAPI インターフェイス*にパススルーします。

これにより両方の長所を生かすことができます。GPU ユーザースペースドライバはゲストで実行されるため、あたかも本物のハードウェア GPU を駆動しているかのように、
フルパフォーマンスで実行することができます。また、GPU カーネルドライバはホストで実行されるため、ホストアプリケーションとゲストアプリケーションの間で
共有することができます。単純化したレンダリング操作は次のようになります：

1. アプリケーションは、GL または Vulkan の描画コマンドを発行して画面に描画。ゲームでは 1 フレームあたり何千もの描画コマンドを実行する可能性あり
2. ゲストのユーザー空間 Mesa ドライバはこれらのコマンドを受け取りGPU コマンド構造に変換。これらのコマンド構造は virtio-gpu プロトコルを使用してドライバがホストから事前に直接マッピングした *GPU メモリに直接書き込まれる*ため、ネイティブで実行するのと同等な速度
3. アプリケーションは描画ストリームをフラッシュするか、GPU にレンダリング開始を要求
4. ユーザー空間の Mesa ドライバは、ホスト上でネイティブに実行する場合と同じように、全体的なレンダリング情報を準備
5. 次に Mesa はコマンドを virtio-gpu *実行バッファ*にラップし、コマンドをホストにパススルーするのに必要な情報を保持
6. コマンドはせいぜい1～2キロバイトのデータで、ゲストカーネルの virtio-gpu ドライバを経由してホストの仮想マシンモニタに入力
7. VMM はコマンドを受け取って [virglrenderer](https://gitlab.freedesktop.org/asahi/virglrenderer) に送り、virglrenderer はネイティブの UAPI GPU コマンド構造をアンラップ
8. virglrenderer は `ioctl()` を発行してホスト GPU カーネルドライバにコマンドに送る
9. ホスト GPU カーネルドライバはコマンドを GPU に送信

ホスト上でネイティブにレンダリングするプロセスは、ステップ 5 から 7 を削除する以外はまったく同じです（ステップ 8 では、Mesa は `ioctl()` を直接発行します）。
したがって、オーバーヘッドは非常に小さくなります。GL または Vulkan コマンドストリーム全体を通過させる代わりに、ゲストからホストへの余分な通信はわずかな
データ量にしか存在しないからです。また、ネイティブとまったく同じドライバを実行できるため、GPU ドライバの品質と互換性を維持したまま、GPU の機能とまったく同一にできます。

セキュリティと分離についてはどうでしょうか？ゲスト上の各プロセスは、ホストカーネルドライバの観点からは独立した GPU プロセスとして扱われます
（より具体的には、ゲストプロセスが仮想 GPU デバイスノードを開くたびに、ホスト VMM はそのために実際の GPU デバイスノードを別のファイル記述子として開きます）。
つまり、ゲスト GPU プロセスは、ネイティブ GPU プロセスが互いに隔離されているのと同じように隔離されています。

今日のところ、DRM Native Contextは [freedreno](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/14900) の上流にしかありません！
しかし、[Intel](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/29870) と [AMD](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/21658) 
GPU 向けの PR が進行中であり、これは Linux ホスト上の Linux ゲスト向けの GPU 仮想化の未来だと本当に思っています(そしておそらくいつの日か、Windows 上で Mesa を実行する
Windows ゲストでも）！

10月のゲームアップデートで DRM Native Context を公開しました。

では、何か問題は？ご想像の通り、性能は*全く*ネイティブではありません。性能への影響は *同期(synchronization)* に起因します。GPU　へのコマンド送信には
少々時間がかかり、GPUコマンドの完了を待つのにも少々時間がかかります。一度にたくさんのGPU作業を 『投げっぱなし(fire-and-forget)』するアプリケーションは、
ほぼネイティブな性能を示しますが、GPU作業が完了するのを頻繁に待つアプリケーションは、待ち時間の増加とスループットの低下に悩まされます。複数のGPUキューを使用し、
それらの間で同期をとるアプリケーションも、VM ゲストを介して同期が行われるため、現在苦しんではいますが、GPU 間の同期をホストカーネルに移行することで、
ネイティブアプリケーションと同様にこれを改善する予定です（まだ実装していないだけですよ）。

しかし、ゲストで GPU レンダリングを動作させるのは話の半分に過ぎません…

## ウィンドウシステム攻撃

DRM Native Context は自身でゲストを GPU にレンダリングさせますが、ディスプレイについてはまだ話していません！伝統的な VM のように仮想スクリーンデバイスを使って
ゲストをウィンドウの中に独立したデスクトップとして表示させることは可能でしょうが、私たちのスタックで必要とすることではありません。VM は薄い層にし、アプリケーションは
ホストのウィンドウシステム上にネイティブに表示されるようにしたいのです。

muvm と libkrun のグラフィック対応がコードを共有する ChromeOS の VMM である Crosvm は、*クロスドメインコンテキスト(cross-domain contexts)* と呼ばれる機能を使ってこれを実現します。
virtio-gpu はゲストに通信チャネルを開かせ、ホスト VMM はそれをホストコンポジタに転送することが可能です。Crosvm　はこれを使って、*sommelier*　というゲストの　Wayland　
コンポジターを使って[Wayland 転送](https://crosvm.dev/book/devices/wayland.html)を実装します。sommelier はゲストの Wayland ソケットを
リッスンし、リクエストをホストの Wayland コンポジタに転送します。プロトコルを転送するだけでは不十分です（それ以外は、virtio vsock や TCP などを使って実現できます）。
sommelier は Wayland コマンドストリームの一部として渡されるバッファファイルディスクリプターも抽出し、virtio-gpu メカニズムを使ってホストとゲスト間で*直接*共有する
必要があります。これによりウィンドウフレームバッファが共有され、余分なコピーコストなしにホスト上で合成できるようになります。ホスト VMM 側では、
これは[rutabaga_gfx](https://github.com/containers/libkrun/tree/v1.9.5/src/rutabaga_gfx/src/cross_domain)のクロスドメインコンポーネントによって処理されていて、
Wayland プロトコルを転送し共有バッファをホスト Wayland コンポジターが理解できる fds(訳注: ファイルディスクリプタ) に変換するコマンドを実装します。

一つ追加の問題がありました。同期です。ゲスト上の GPU アプリがバッファにレンダリングするとき、レンダリングの終了を待つ前にそれを直接ホストに渡します。
 *暗黙的同期（implicit sync)* と呼ばれるメカニズムが、カーネルに同期を直接処理させることになっています。カーネルは、バッファ上でどの GPU 操作が保留されているかを知っており、
別のコンテキストでバッファから読み込む前に、それらが完了するまでGPUを待たせます。暗黙的同期はレガシー機能と考えられているため、私たちの GPU カーネルドライバはこれを実装せず、
互換性のために Mesa で実装しています（Nvidia はこれを拒否しており、Wayland が Nvidia GPU 上で長い間壊れていた理由はこれです… ）。私たちの Mesa ドライバは、
共有バッファがレンダーターゲットであるときに暗黙の同期情報（*GPU fence*）を共有バッファに自動的に挿入し、バッファから読み出す前にもう一方の fence を取り出します。
これはホストとゲストの両方でうまくいきましたが、ホストとゲストの間で共有されているバッファではうまくいきませんでした。ゲスト Mesa がゲストバッファに fence を挿入すると、
その fence はゲストカーネルにのみ登録されるため、ホストカーネルは進行中のレンダリングについて何も知らず、必要なときにホスト Mesa に fence を引き渡すことができませんでした。
そのため、ティアリングや視覚的な不具合や『フレームラグ』が発生しました。これを解決するために、結局、暗黙的同期に戻しました。 Asahi virtgpu プロトコルは、『古典的な』
暗黙的同期ドライバと同じようにサブミッションを持つバッファリストを含み、それらのバッファはホスト VMM の virglrenderer コードで fence が登録されます。
暗黙的同期互換性コードは、すべてのベースをカバーするために、ゲスト Mesa とホスト VMM で2回実行されます。将来的には、明示的同期(explicit sync)を適切に実装し、
クロスドメインのものと統合し、このハックを削除する予定ですが、上流には存在しないゲストカーネルのパッチが必要になるため、少々お待ちください。話がそれました。

X11 についてはどうでしょう？ほとんどのゲームは X11 を必要とするので、sommelier はゲスト内で独自の *XWayland* インスタンスを実行して X11 対応を提供できます。簡単ですよね？

この解決策を10月に公開したのですが… あまりうまくいきませんでした。

## sommelier の悩み

結局のところ、sommelier は我々が期待していたほど素晴らしい解決策ではありませんでした。これは ChromeOS ホストで使うように設計されており、より
一般的なLinuxデスクトップではあまりうまく動作しません。

sommelier は*多すぎるの*と*少なすぎるの*の両方です。非常に複雑なコードでできていて、Wayland プロトコルの解析やバッファのコピーなどをたくさんのことを行います。
つまり、ゲスト上のアプリケーションは、ホスト上でネイティブに動作するアプリケーションのようには動作しません。DPI のスケーリングやウィンドウ/ポップアップの配置など、
多くの問題がありました。同時に、sommelier は XWayland と統合するのに十分なことをしていません。KDE Plasma のような一般的な Wayland デスクトップ環境では、
コンポジタは X11 アプリケーションと適切に統合するために多くの余分な作業を行い、XWayland と直接会話してそれらを管理しています。これは sommelier ではこれらの
作業を行っておらず素の X11 統合しか提供していません。X11 アプリにはタイトルバーがなく、メニューは正しく機能せず、クリップボードは正しく機能せず、システムトレイとの
統合はなく、いくつかのウィンドウは奇妙に透けて表示されます…

その上、sommelier は C で書かれており、クライアント接続を完全に分離していません。一般的には XWayland クライアントで十分に動作するが、おかしなことがあると
クラッシュして VM 環境全体がダウンすることもあります。ネイティブの Wayland プロキシは問題外でした。 試してみましたが、壊れすぎていてまったく使えませんでした。

そこで、sommelier と XWayland を X11 のみのソリューション（Wayland ソケット非対応）として公開し、フルスクリーンアプリは一般的に十分に機能するため、
ゲームのみのソリューションとして宣伝することにしました。これは、Steam を Big Picture モードで動かすことを意味しました（ウィンドウモードはかなり壊れていたためです）。
また、ウィンドウランチャを使うゲームは使えないことが多かったです。

しかし、もっと良い解決策が必要です。

## X11が先？

muvm が muvm と呼ばれるようになる前に、virtio の vsock socket で X11 を直接転送する実験をしたことがあります。これはネットワークや SSH トンネル経由で X11 を
実行するのと同じように動作します。この欠点は、GPU アクセラレーションが動作しないことです。このような『ダム(dumb)』ソケット上で GPU バッファを通過させる方法が
ないからです。それでも、ソフトウェアレンダリングを強制しながらいくつかのアプリを動かしてみると、ゲーム以外のアプリではこちらの方がはるかに優れたソリューションで
ることは明らかでした。X11 アプリはホスト上とまったく同じように動作し、ウィンドウ管理の問題もなく、クリップボードやトレイの統合なども適切に動作しました。
このままゲーム以外のアプリの代替品として公開する準備はできていましたが、GPU　アクセラレーションを動作させるのはどれだけ大変ななんでしょう……
と思っていたら、[chaos_princess氏](https://social.treehouse.systems/@chaos_princess) が *x112virtgpu* というものを出してきました。

```
Wayland が未来なのに、なぜ X11 に注目するのですか？端的に言えば、みんなが実行したいと思う x86/x86-64 アプリの
ほとんどは、Wayland 対応ビルドされていないか、Wayland 以前のものだからです。Asahi Linux 上で動作するネイティブ
デスクトップ環境では Wayland を全面的に採用していますが、XWayland はまだ完全にサポートされていますし、
エミュレーション下で動作するゲームやレガシーアプリケーションでは、X11 プロトコルのサポートを優先するのが
賢いやり方です。10月に公開したソリューションは、Wayland を使用しているにもかかわらず、Wayland アプリケーションに
対応しませんでした（有効にしようとしましたが、本当に壊れていました）。そのため、今のところ X11 のみのソリューションに
切り替えても、後退することにはなりません。
```

x112virtgpu は現在 muvm-x11bridge に改名され、まさに sommelier がそうあって欲しかったものになりました。 X11 プロトコルの薄いプロキシで、クロスドメインチャンネルを
使用して、virtgpu バッファ共有を使用してフレームバッファをコピーせずにホスト X サーバに直接転送します。sommelier とは異なり、特別な処理が必要なコマンドを抽出するために
必要以上に X11 プロトコルを解釈しようとしません。つまり、直接 X11 パススルーするのと同じように動作し、さらに GPU アクセラレーションとバッファ共有も行います。

… いずれにせよこれが理論です。しかし、1つだけ重大な問題があります…

## Futex とは何ですか？

X11 プロトコルはかなり*特殊*です。ファイルディスクリプタを使ってフレームバッファを通過させるだけでなく、*fence*という別のものも通過させます。いや、最新の GPU の
明示的同期 fence（ごく最近のX11ではそれも対応していますがそれはまた別の話）ではありません。CPU 側の fence です。Futex を使って実装されています。

[Futex](https://en.wikipedia.org/wiki/Futex) は、Linux上でクロススレッドおよびクロスプロセス同期を行うためのカーネルシステムコールです。基本的には、
自分でミューテックスを作成するためのプリミティブです。2つのスレッドまたはプロセスは、*メモリを直接共有*し、アトミック（訳注: [不可分操作](https://ja.wikipedia.org/wiki/%E4%B8%8D%E5%8F%AF%E5%88%86%E6%93%8D%E4%BD%9C))を使用して同期することになっています。
あるプロセスが他のプロセスを待つ必要があるときは、カーネルの `futex()` システムコールを使って自分自身をスリープ状態にし、他のプロセスが元のプロセスを起床させる
必要があるときは、同じシステムコールを使って起床させます。カーネルは、システムコールが共有メモリの実際の値と同期していることを確認するので、競合状態は発生しません。

ホストとゲストのプロセス間でこれを機能させる必要があります… しかし、クロスドメインの `futex()` システムコールというものは存在しません。ホストとゲストのカーネルは、
お互いのfutexについて何も知りません。そして、これを動作させるためには、メモリを共有する必要があります…

X11 プロトコルをもっと読み解き futex fence が何のために使われるのかを正確に理解することによって、この問題を高レベルで解決することが*できます*。
ホスト VMM と x11bridge の各サイドは、どちらのサイドが futex のシグナル送信(signaling)と待機(wait)を担当するかを理解しなければなりません。
そして、シグナルを送る側のプロキシコンポーネントはシグナルを待ち、クロスドメインコマンドを送信してシグナルをもう一方の側に転送しなければなりません。
これは*機能する*と思われますが、*手間*がかかるし、余計な待ち時間が増えることになります。 futex　の仕組みは、一方がもう一方を待っていなければ、
アトミック操作を直接使うことで、システムコールを一切使わずに動作することになっています。

そこで chaos_princess氏は、別のことを試してみることにしました。もし実際にメモリを共有できるとしたらどうでしょうか？ホストプロセスとゲストプロセスが、
共有メモリバッファを実際に一緒にマッピングすることができれば、あたかも同じ側で実行されているかのようにアトミックを使うことができ、`futex()` システムコールの
使い方を何らかの方法でプロキシするだけでよくなります。しかし、メモリを共有するにはどうすればいいのでしょうか？GPU バッファを使うという手もあるが、
ちょっとやりすぎな気がするし、X サーバやクライアントと直接統合するのは無理があります…

X11 の共有メモリ fence を扱うライブラリは *libxshmfence* です。このライブラリは、メモリを共有するために複数のメカニズムを使うことができます。
Linuxの場合、[デフォルトはmemfd()](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/blob/libxshmfence-1.3.2/src/xshmfence_alloc.c?ref_type=tags#L78)で、
これは『メモリ内(in-memory)』ファイルを作成するシステムコールです。VM の境界を越えて、これらのファイルを共有できるのでしょうか？残念ながら、簡単ではありません。

しかし、別の方法があります。

muvm では、はじめからホストのファイルシステムを直接ゲストと共有しています。これは基本的に、VM にプロキシされた *FUSE* ファイルシステムです。通常、
これはファイルの読み書き時にデータをコピーすることで動作します。しかし、*virtiofs DAX* と呼ばれる特別なモードがあり、ゲストがホストのファイルを
直接メモリにマップすることができます！ファイルシステムが DAX で共有されマウントされると、ホストとゲストで同じファイルの `mmap()` が実際に直接メモリを共有し、
アトミック操作がゲストとホストの間で動作します！なんてクールなんでしょう？

結局のところ、DAX は新しく少し壊れてるので、libkrun とゲストカーネルの両方にいくつかのパッチを送って、確実に動くようにする必要がありました
（そのうちのいくつかを上流に送る方法をまだ考え中です…）。muvm は現在、DAX 対応を使ってゲストの `/dev/shm` をマウントすることで、ホストとゲストが
POSIX 共有メモリを共有できるようにしています。

しかし...libxshmfence は依然として memfds の使用を続けています。どうすればこれを直せるんでしょうか？

chaos_princess氏 は、その名前に忠実に、*ptrace* という解決策で完全なカオスモードに入ることにしました！muvm-x11bridge がクライアントプロセスから memfd を受け取ると、
それを *ptrace し、必要なシステムコールを注入して /dev/shm ファイルをオープンし、何事もなかったかのようにファイルディスクリプタをスワップアウトします。*
次に、ファイル名をホストに渡すだけで、libkrun は開いて何事もなかったかのように fd(訳注:ファイルディスクリプタ) を X サーバーに渡すことができます。

それから、muvm-x11bridge は小さな futex 監視スレッドを実行し、クライアントプロセスからの `futex()` 起床(wake up)コールを検出し、起床をホストに送信し、X サーバに
転送します。

狂っていますね！でもうまくいきました…ような？ `ptrace()`ダンスは非常に繊細で、うまく動作させるのに何度も試行錯誤しました。`futex()` プロキシもまた、正しく動作させるのが
かなり難しいことがわかりました。最終的にはかなり確実に動作するようになりましたが、重大な制限があります。 `muvm-x11bridge` が壊れてしまうので、X11 クライアントアプリで 
ptrace を他の目的で使うことができません。つまり、デバッグのために *strace* を使うことができず、ptrace を好んで使うような複雑なアプリ（Chromium や Chromium ベースの
アプリなど）では問題が発生します。

`LD_PRELOAD` を使って libxshmfence をハイジャックすることも考えられるが、これも chaos_princess氏 が実装した解決策です… しかし、3つのアーキテクチャ
（ネイティブの arm64、エミュレートされた x86、エミュレートされた x86-64）にまたがってこの作業を行うので、3つのバージョンのライブラリを公開しなければ
なりませんし、また、このソリューションがコンテナ技術で正しく機能することは保証されていません。

本気でこれを公開したいならば、もっと良い解決策が必要です…

## Futex テイク2

libxshmfence のコードをよく見てみると、`memfd()` が動作しない場合の[フォールバックパスがいくつかあること](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/blob/libxshmfence-1.3.2/src/xshmfence_alloc.c?ref_type=tags#L82)がわかりました。
まず、`shm_open(SHM_ANON, ...)` を使いますが、これは BSD のもので、Linux では使えません。次に、`O_TMPFILE` で `open()` を試みますが、`O_TMPFILE` は
virtiofs では対応していません。最後に、`/dev/shm` にファイルをオープンしてリンクを解除するという、昔ながらの方法で実行します。`memfd()` を使わないようにできれば、
まさに私たちが望むことができるでしょう。できますかねえ？

[seccomp-bpf](https://www.kernel.org/doc/html/v5.1/userspace-api/seccomp_filter.html)フィルターを使って `memfd()` システムコールを無効にするなど、
いくつか試してみました(訳注: [seccompの解説](https://mmi.hatenablog.com/entry/2016/08/01/044000)。残念ながら、これは FEX のコードパスのように、
`memfd()` が存在し、動作していると仮定しているもの (最近のLinuxカーネルでは、かなり合理的な仮定です)を壊してしまいます。

それで… 結局、環境変数を使って `memfd()` の使用を無効にできるよう、libxshmfence にちょっとした[マージリクエスト](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/merge_requests/9)を送ってみましたが…　マージされました！

このパッチはまだ新しい libxshmfence バージョンとしてリリースされていないので、新しい muvm バージョンを出荷したいディストリビューションは、信頼性の低い ptrace() ハックに
逆戻りしないように、このパッチを libxshmfence パッケージにバックポートする必要があります。

```
ありがたいことに、これらのフェンスは常に X11 クライアント上で作成され、X サーバーに共有されるので、ゲストの X11 クライアントだけが
ハックを必要とし、X サーバーは余分な環境変数やパッチなしで libxshmfence を使うことができます。
```

これで完了でしょうか？ちょっと違います…　libxshmfence はいまだ共有メモリファイルを削除し、fd だけを X11 サーバーに渡します。これは望ましいことだし
（エラー時に古いファイルが残るリスクを回避できる）、libxshmfence にこれ以上ハックを増やしたくないからです… では、ファイルをリオープンできない場合、
どうやってホスト側に fd を渡せばいいのでしょうか？

さて… 考えてみると、たとえファイルが削除されたとしても、ゲストで開いているすべての fd は実際にはホストの VMM によって*開かれています*。これが virtiofs の
仕組みです！つまり、ホストは*すでに*必要なファイルディスクリプタをどこかに持っています… それを見つけるだけでいいのです。これを動作させるために、libkrun の 
virtiofs 実装に魔法の fd-export `ioctl()`を追加しました。これは GPU バッファのエクスポートのようなもので、X11 クロスドメインプロトコルの一部として
ホストに渡せる識別子を与えます。このプロトコルにより、*エクスポートされたファイルディスクリプタ*の特別なテーブルを参照して、基盤となるホストの fd を
見つけることができます。これは virtiofs コードと rutabaga_gfx クロスドメインコードの間でexported-fd テーブルを共有することで実装されています。
このコードを通すにはそれなりの数のレイヤーが必要なんです… でも、Sergio 氏はこの解決策に十分満足し、[PR　をマージ](https://github.com/containers/libkrun/pull/231)してくれました ^^

```
このトリックは、複数の独立したクロスドメインクライアント（おそらく異なるユーザーとして実行）が互いの fd を盗む可能性があるという
意味ではセキュアではありません、いずれにせよ muvm-x11bridge のインスタンスは1つしかなく、microVM 全体が単一ユーザーの
パーミッションのコンテキストで実行されるため、私たちのユースケースには影響しません。それに、virtgpu クロスドメインが今日
意味のあるパーミッションチェックをするとは思えません。
```

最終的に、futex とクロスドメインの X11 コードのデバッグを重ねた後、ようやく安定し、マージしてリリースする準備ができました！

## Waylandについてはどうですか？

将来的には、ネイティブの Wayland パススルーにも対応したいと考えています。そのためには muvm-x11bridge と対になる Wayland が必要です。
sommelierと違って、これはずっとシンプルで同じ哲学に従う予定であり、X11 パススルーと同じように動作すると期待しています。しかし、
fence と明示的同期を先に解決すべきなので、純粋な Wayland パススルーは今のところToDoリストに残るでしょう… 良いニュースは、奇妙な 
futex のようなものが存在しないことです！

## Muvm、リロード

今日、`sudo dnf update --refresh` で Fedora Asahi Remix システムをアップデートすると、x86 エミュレーションスタックが10月のリリースより改善されます。

### muvm の変更

- `muvm-x11bridge` で直接X11パススルー（`--sommelier`でsommelierに戻せるがなんでそうするの？）
  - Steam　やその他のウィンドウアプリが正しく動作するようになり、システムトレイアイコンも動作するようになりました。
  - また、ホスト上で正しく設定されていれば、古き良き　XIM　を使って Steam で日中韓の入力メソッドを使用可能に！(KDE Plasma 上の fcitx でテスト)
- `muvm-hidpipe` が統合されたので、私たちの`steam`ラッパーを使わなくてもゲームパッドが動作可能に。実際、Steam を一度インストールすれば、`muvm -- FEXBash -c '~/.local/share/Steam/steam.sh'` で機能を損なうことなく実行可能
- `/dev/shm` がホストとゲストで共有されるようになり、興味深い使用例が可能に
- ゲストのメモリがより適切に管理されるようになり、ホストの OOM が減るはず
- 8GB マシンのデフォルトのメモリ割り当てが高くなり、より多くのゲームがプレイ可能に
- すでに稼働しているVMの中でコマンドを実行するインタラクティブモード。例えば、`muvm -ti -- bash` を実行するとシェルが起動
- また、root-server ポートに接続して root としてコマンドを実行することも可能に: `muvm -tip 3335 -- bash`
- 小さな改善と修正がたくさん

### FEX の変更

- 新しい Steam CEF アップデートの修正を含む、[多くのバグフィックスと改善](https://fex-emu.com/FEX-2412/)

### virglrendererとMesaの変更

- [Vulkan 1.4 をサポート](https://social.treehouse.systems/@AsahiLinux/113584784231664115)
- 多くのバグフィックスとパフォーマンスの改善

### Steamラッパーの変更

- Steam が Big Picture モードではなく、通常のウィンドウモードで起動

![steam im](https://asahilinux.org/img/blog/2024/12/steam-im.png)
日本語入力が可能な標準ウィンドウ UI の Steam

また、Steam 以外のアプリケーションを実行するために必要なビットのインストールも自動化されているので、Steam を実行したくない場合は、Steam パッケージは必要ありません。
上記のようにまずシステムをアップデートし、それから `sudo dnf install fex-emu` をインストールするだけで、`muvm` で x86 および x86-64 アプリケーションを
実行するのに必要なものがすべて手に入ります。

ウィンドウ管理がそれなりにうまくいくようになったので、ゲーム以外のアプリも試してみることをお勧めします。一般に、スタンドアロンの tarball としてパッケージ化された
アプリケーション（複雑な OS 依存関係や他のアプリケーションとの相互作用がないもの）は、AppImages を含めてうまく動作する可能性が高いです。
Flatpak は FEX/muvm と統合されていないので、x86-64 の Flatpak はまだ動作しないことに注意してください。どの程度動作するか、ぜひ教えてください！

#### Asahi Lina · 2024-12-12
