[Beyond Gaming: X11 bridging in muvm](https://asahilinux.org/2024/12/muvm-x11-bridging/)の非公式日本語訳です。

まだDeepLの結果を貼っただけ
訳注: 本家ブログへのリンクは対応する日本語訳へのリンクに変更

---
# ゲームを超えて: muvmによる X11 ブリッジ
- [前回](https://github.com/asfdrwe/asahi-linux-translations/blob/main/PROGRESS202410.md)

皆さん、こんにちは！Asahi Linuxのx86/x86-64エミュレーション・スタックにとてもクールなアップデートがありましたので、
その内容をシェアしたいと思います。今日現在、ゲーム以外のアプリが使えるようになりました！


![cisco-pt](https://asahilinux.org/img/blog/2024/12/cisco-pt.png)

Fedora Asahi Remix上で動作するCisco Packet Tracer

## VMにおけるネイティブ・グラフィックス

[以前のブログ記事](https://github.com/asfdrwe/asahi-linux-translations/blob/main/PROGRESS202410.md)を覚えているかもしれないが、Asahi Linuxは[muvm](https://github.com/AsahiLinux/muvm)によって駆動されるmicroVMですべてのx86/x86-64アプリケーションを実行している。ハードウェアGPUパススルーなしで、ネイティブに近いパフォーマンスで実際のVMでゲームを実行するにはどうすればいいのだろうか？

AMD/Intelシステムでは（そして実際、Apple Silicon上のmacOSでも）、GPU仮想化は常にある種の制限を受けてきた。選択肢は、ハードウェアGPUを完全にパススルーするか、APIレベルのGPU準仮想化を使うかです。ハードウェアGPUを通すと、GPUデバイスを完全にゲストに割り当てます。これは、そのGPU用の本物のドライバを持つすべてのゲストOSで動作し、ネイティブなパフォーマンスを持ちますが、GPUがゲスト専用であることを意味し、ホストと共有したり、ゲストとホストのウィンドウを1つの画面に統合したりすることはできません。一方、APIレベルのGPU準仮想化は、基本的にOpenGL、Vulkan、またはMetalコマンドをゲストからホストに送信し、ホストのGPUドライバスタックによって処理されます。これはゲストに 「汎用 」準仮想化 GPU ドライバを必要とし、すべての高レベル GPU 描画コマンドがゲストからホストへの障壁を越え、ホスト上で処理されなければならないため、ネイティブ GPU の使用よりもはるかに遅くなります。これが macOS での GPU 仮想化の仕組みです。一部のGPU（例えば最近のNvidia GPU）はハードウェア仮想化サポートで真の共有をサポートしていますが、これはまだアップストリームではなく、ハードウェアのサポートが必要で、Apple GPUにはありません。

しかし、もっと良い方法があるとしたら？それがわかった！[DRMネイティブ・コンテキスト](https://indico.freedesktop.org/event/2/contributions/53/attachments/76/121/XDC2022_%20virtgpu%20drm%20native%20context.pdf)の登場だ。

コンセプトはとてもシンプルです： ゲスト上でGPUドライバスタック全体を実行し、ハードウェアGPU全体を通過させる代わりに（共有なし）、またはホスト上でGPUドライバスタック全体を実行し、高レベルAPIを通過させる代わりに（遅い）、それを真ん中で分割しませんか？DRM Native Contextは、ホスト上でGPUカーネルドライバを実行し、ゲスト上でGPUユーザースペースドライバ（Mesa）を実行し、ゲストからホストへカーネルUAPIインターフェイスを通過させます。

GPU ユーザースペースドライバはゲストで実行されるため、あたかも本物のハードウェア GPU を駆動しているかのように、フルパフォーマンスで実行することができます。また、GPUカーネルドライバはホストで実行されるため、ホストアプリケーションとゲストアプリケーションの間で共有することができます。単純化したレンダリング操作は次のようになります：

1. アプリケーションは、GLまたはVulkanの描画コマンドを発行して画面に描画します。ゲームでは、1 フレームあたり何千もの描画コマンドを実行するかもしれません。
2. ゲストのユーザー空間 Mesa ドライバはこれらのコマンドを受け取り、GPU コマンド構造に変換します。これらのコマンド構造は、ドライバが virtio-gpu プロトコルを使用してホストから直接マッピングした GPU メモリに直接書き込まれるため、ネイティブで実行するのと同じ速度になります。
3. アプリケーションは描画ストリームをフラッシュするか、GPU にレンダリング開始を要求します。
4. ユーザー空間の Mesa ドライバは、ホスト上でネイティブに実行する場合と同じように、全体的なレンダリング情報を準備します。
5. 次に Mesa はコマンドを virtio-gpu 実行バッファにラップし、コマンドをホストに渡すために必要な情報を持ちます。
6. コマンドはせいぜい1～2キロバイトのデータで、ゲストカーネルの virtio-gpu ドライバを経由してホストの仮想マシンモニタに入ります。
7. VMMはコマンドを受け取って[virglrenderer](https://gitlab.freedesktop.org/asahi/virglrenderer)に渡し、virglrendererはネイティブのUAPI GPUコマンド構造をアンラップします。
8. virglrenderer は ioctl() を発行してホスト GPU カーネルドライバにコマンドを渡します。
9. ホスト GPU カーネルドライバはコマンドを GPU に送信します。

ホスト上でネイティブにレンダリングするプロセスは、ステップ 5 から 7 を削除する以外はまったく同じです（ステップ 8 では、Mesa は ioctl() を直接発行します）。したがって、GL または Vulkan コマンドストリーム全体を通過させる代わりに、ゲストからホストへの余分な通信はわずかなデータ量にしか関与しないため、オーバーヘッドは非常に小さくなります。また、ネイティブとまったく同じドライバを実行できるため、GPUドライバの品質と互換性を維持したまま、GPUの機能をまったく同じにすることができます。

セキュリティと分離についてはどうですか？ゲスト上の各プロセスは、ホストカーネルドライバの観点からは独立した GPU プロセスとして扱われます（より具体的には、ゲストプロセスが仮想 GPU デバイスノードを開くたびに、ホスト VMM はそのために実際の GPU デバイスノードを別のファイル記述子として開きます）。つまり、ゲスト GPU プロセスは、ネイティブ GPU プロセスが互いに隔離されているのと同じように隔離されています。

今日のところ、DRMネイティブコンテキストは[freedreno](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/14900)のアップストリームにしかありません！しかし、[Intel](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/29870)と [AMD](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/21658) GPU 向けの PR が進行中であり、これは本当に Linux ホスト上の Linux ゲスト（そしておそらくいつの日か、Windows 上で Mesa を実行する Windows ゲスト）向けの GPU 仮想化の未来だと思います！

私たちは10月のゲーム・アップデートでDRMネイティブ・コンテキストを出荷しました。

では、何が問題なのでしょうか？ご想像の通り、パフォーマンスはネイティブではありません。パフォーマンスへの影響は同期に起因します： GPUへのコマンド送信には少し時間がかかり、GPUコマンドの完了を待つのにも少し時間がかかります。一度にたくさんのGPU作業を 「fire-and-forget 」するアプリケーションは、ほぼネイティブな性能を示しますが、GPU作業が完了するのを頻繁に待つアプリケーションは、待ち時間の増加とスループットの低下に悩まされます。複数のGPUキューを使用し、それらの間で同期をとるアプリケーションも、VMゲストを介して同期が行われるため、現在苦しんではいますが、GPU間の同期をホストカーネルに移行することで、ネイティブアプリケーションと同じようにこれを改善する予定です（まだ実装していないだけです）。

しかし、ゲストでGPUレンダリングを動作させるだけでは、話の半分に過ぎません...。

## ウィンドウシステムの攻撃

DRM ネイティブコンテキストはそれ自体でゲストを GPU にレンダリングさせますが、ディスプレイについてはまだ話していません！伝統的なVMのように仮想スクリーンデバイスを使い、ゲストをウィンドウの中に独立したデスクトップとして表示させることは可能でしょうが、私たちのスタックではそれは望みません。VMは薄いレイヤーにし、アプリケーションはホストのウィンドウシステム上にネイティブに表示されるようにしたいのです。

muvmとlibkrunのグラフィックサポートがコードを共有するChromeOSのVMMであるCrosvmは、クロスドメインコンテキストと呼ばれる機能を使ってこれを実現します。virtio-gpuはゲストに通信チャネルを開かせ、ホストVMMはそれをホストコンポジターに転送できます。Crosvmはこれを使って、sommelierというゲストのWaylandコンポジターを使って[Wayland転送](https://crosvm.dev/book/devices/wayland.html)を実装します。sommelierはゲストのWaylandソケットをリッスンし、リクエストをホストのWaylandコンポジターに転送します。sommelier は Wayland コマンドストリームの一部として渡されるバッファファイルディスクリプターも抽出し、virtio-gpu メカニズムを使ってホストとゲスト間で直接共有する必要があります。これによりウィンドウフレームバッファが共有され、余分なコピーコストなしにホスト上で合成できるようになります。ホストVMM側では、これは[rutabaga_gfx](https://github.com/containers/libkrun/tree/v1.9.5/src/rutabaga_gfx/src/cross_domain)のクロスドメインコンポーネントによって処理され、Waylandプロトコルを転送し、共有バッファをホストWaylandコンポジターが理解できるfdsに変換するコマンドを実装します。

ひとつだけ、余分なキャッチがあった： 同期だ。ゲスト上のGPUアプリがバッファにレンダリングするとき、レンダリングの終了を待つ前にそれを直接ホストに渡します。暗黙的同期（implicit sync）と呼ばれるメカニズムが、カーネルに同期を直接処理させることになっています： カーネルは、バッファ上でどのGPU操作が保留されているかを知っており、別のコンテキストでバッファから読み込む前に、それらが完了するまでGPUを待たせます。カーネルはバッファ上でどのGPU操作が保留されているかを知っており、別のコンテキストから読み込む前に完了するまでGPUを待たせます。暗黙的同期はレガシー機能と考えられているため、私たちのGPUカーネルドライバはこれを実装せず、互換性のためにMesaに実装しています（Nvidiaはこれを拒否しており、これがWaylandがNvidia GPU上で長い間壊れていた理由です...）。私たちのMesaドライバは、共有バッファがレンダーターゲットであるときに暗黙の同期情報（GPUフェンス）を共有バッファに自動的に挿入し、バッファから読み出す前にもう一方のフェンスを取り出します。これはホストとゲストの両方でうまくいきましたが、ホストとゲストの間で共有されているバッファではうまくいきませんでした： ゲストMesaがゲストバッファにフェンスを挿入すると、そのフェンスはゲストカーネルにのみ登録されるため、ホストカーネルは進行中のレンダリングについて何も知らず、必要なときにホストMesaにフェンスを引き渡すことができませんでした。そのため、ティアリングや視覚的な不具合、「フレームラグ」が発生した。これを解決するために、結局、暗黙的同期に戻しました： Asahi virtgpuプロトコルは、「古典的な 」暗黙的同期ドライバと同じようにサブミッションを持つバッファリストを含み、それらのバッファはホストVMMのvirglrendererコードでフェンスが登録されています。暗黙同期互換性コードは、すべてのベースをカバーするために、ゲストMesaとホストVMMで2回実行されます。将来的には、明示的同期を適切に実装し、クロスドメインのものと統合し、このハックを削除する予定ですが、アップストリームではないゲストカーネルのパッチが必要になるため、少し待たなければなりません。

X11についてはどうだろう？ほとんどのゲームは X11 を必要とするので、sommelier はゲスト内で独自の XWayland インスタンスを実行して X11 サポートを提供できる。簡単でしょう？

私たちはこのソリューションを10月に出荷したのだが...あまりうまくいかなかった。

## ソムリエの悩み

結局のところ、sommelierは我々が期待していたほど素晴らしいソリューションではない。これはChromeOSホストで使うように設計されており、より一般的なLinuxデスクトップではあまりうまく動作しない。

sommelierは多すぎるし、少なすぎる。かなり複雑なコードで、Waylandプロトコルの解析やバッファのコピーなどをたくさん行う。つまり、ゲスト上のアプリケーションは、ホスト上でネイティブに動作するアプリケーションのようには動作しません。DPIのスケーリングやウィンドウ/ポップアップの配置など、多くの問題がありました。同時に、sommelierはXWaylandと統合するのに十分なことをしていない。KDE Plasmaのような一般的なWaylandデスクトップ環境では、コンポジターはX11アプリケーションと適切に統合するために多くの余分な作業を行い、XWaylandと直接会話してそれらを管理している。これはsommelierでは機能せず、素のX11統合しか提供しない。X11アプリにはタイトルバーがなく、メニューは正しく機能せず、クリップボードは正しく機能せず、システムトレイとの統合はなく、いくつかのウィンドウは奇妙に透けて表示される...。

その上、sommelierはCで書かれており、クライアント接続を完全に分離していない。一般的にはXWaylandクライアントで十分に動作するが、おかしなことがあるとクラッシュしてVM環境全体がダウンすることもある。ネイティブのWaylandプロキシは問題外でした： 試してみましたが、壊れすぎていてまったく使えませんでした。

そこで私たちは、sommelierとXWaylandをX11のみのソリューション（Waylandソケットをサポートしない）として出荷し、フルスクリーンアプリは一般的に十分に機能するため、ゲームのみのソリューションとして宣伝することにしました。これは、Steamをビッグピクチャーモードで動かすことを意味した（ウィンドウモードはかなり壊れていたため）。また、ウィンドウ・ランチャーを使うゲームは使えないことが多かった。

しかし、私たちはより良いソリューションを必要としていました。

## X11が先か？

muvmがmuvmと呼ばれるようになる前に、virtioのvsockソケットでX11を直接転送する実験をしたことがある。これはネットワークやSSHトンネル経由でX11を実行するのと同じように動作します。この欠点は、GPUアクセラレーションで動作しないことだ。このような「ダム」ソケット上でGPUバッファを通過させる方法がないからだ。それでも、ソフトウェアレンダリングを強制しながらいくつかのアプリを動かしてみると、ゲーム以外のアプリではこちらの方がはるかに優れたソリューションであることは明らかだった。X11アプリはホスト上とまったく同じように動作し、ウィンドウ管理の問題もなく、クリップボードやトレイの統合なども適切に動作した。このままゲーム以外のアプリの代替品として出荷する覚悟はできていたが、GPUアクセラレーションを動作させるのがどれだけ大変なことか......と思っていたら、chaos_princessがx112virtgpuというのを出してきた。

```
Waylandが未来なのに、なぜX11に注目するのか？端的に言えば、人々が実行したいと思うx86/x86-64アプリのほとんどは、
Waylandサポート用にビルドされていないか、それ以前のものだからだ。Asahi Linux上で動作するネイティブ・デスクトップ
環境ではWaylandを全面的に採用していますが、XWaylandはまだ完全にサポートされていますし、エミュレーション下で
動作するゲームやレガシー・アプリケーションでは、X11プロトコルのサポートを優先するのが賢いやり方です。
私たちが10月に出荷したソリューションは、Waylandを使用しているにもかかわらず、Waylandアプリケーションを
サポートしませんでした（私たちはそれを有効にしようとしましたが、本当に壊れていました）。
```

x112virtgpuは現在muvm-x11bridgeに改名され、まさにsommelierがそうあって欲しかったものだ： X11 プロトコルのシンプロキシで、クロスドメインチャンネルを使用して、virtgpu バッファ共有を使ってフレームバッファをコピーせずにホスト X サーバに直接転送します。sommelier とは異なり、特別な処理が必要なコマンドを抽出するために必要以上に X11 プロトコルを解釈しようとしません。つまり、直接X11パススルーするのと同じように動作し、さらにGPUアクセラレーションとバッファ共有も行います。

......あるいは、それがとにかく理論的なことだ。しかし、1つだけ重大な問題が...。

## Futexって何？

X11プロトコルはかなり特殊だ。ファイルディスクリプタを使ってフレームバッファを通過させるだけでなく、フェンスという別のものも通過させるのだ。いや、最新のGPUの明示的同期フェンス（ごく最近のX11ではそれもサポートされているが、それはまた別の話）ではない。CPU側のフェンスは、フテックスを使って実装されている。

[Futex](https://en.wikipedia.org/wiki/Futex)は、Linux上でクロススレッドおよびクロスプロセス同期を行うためのカーネル・システムコールである。基本的には、自分でミューテックスを作成するためのプリミティブである。2つのスレッドまたはプロセスは、メモリを直接共有し、アトミックを使用して同期することになっている。あるプロセスが他のプロセスを待つ必要があるときは、カーネルのfutex()システムコールを使って自分自身をスリープ状態にし、他のプロセスがスリープ状態を解除する必要があるときは、同じシステムコールを使ってスリープ状態にする。カーネルは、システムコールが共有メモリの実際の値と同期していることを確認するので、競合状態は発生しない。

しかし、クロスドメインのfutex()システムコールというものは存在しない。ホストとゲストのカーネルは、お互いのfutexについて何も知らない。そして、これを動作させるためには、メモリを共有する必要がある。

X11プロトコルをもっと解釈して、futexフェンスが何のために使われるのかを正確に理解することで、この問題を高レベルで解決することができる。ホストVMMとx11bridgeの各サイドは、どちらのサイドがfutexのシグナリングと待機を担当するかを理解しなければならない。そして、シグナルを送る側のプロキシコンポーネントはシグナルを待ち、クロスドメインコマンドを送信してシグナルをもう一方の側に転送しなければならない。これは機能するだろうが、手間がかかるし、余計な待ち時間が増えることになる： futexの仕組みは、一方がもう一方を待っていなければ、アトミック操作を直接使うことで、システムコールを一切使わずに動作することになっている。

そこでchaos_princessは、別のことを試してみることにした。ホストプロセスとゲストプロセスが、共有メモリバッファを実際に一緒にマッピングすることができれば、あたかも同じ側で実行されているかのようにアトミックを使うことができ、futex()システムコールの使い方を何らかの方法でプロキシするだけでよい。そうすれば、futex()システムコールのプロキシを使えばいいだけだ。GPUバッファを使うという手もあるが、ちょっとやりすぎな気がするし、Xサーバーやクライアントと直接統合するのは無理がある...。

X11の共有メモリフェンスを扱うライブラリはlibxshmfenceです。このライブラリは、メモリを共有するために複数のメカニズムを使うことができる。Linuxの場合、[デフォルトはmemfd()](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/blob/libxshmfence-1.3.2/src/xshmfence_alloc.c?ref_type=tags#L78)で、これは「メモリ内」ファイルを作成するシステムコールだ。VMの境界を越えて、これらのファイルを共有できるのだろうか？残念ながら、簡単ではない。

しかし、別の方法がある。

muvmでは、すでにホストのファイルシステムを直接ゲストと共有している。これは基本的に、VMにプロキシされたFUSEファイルシステムです。通常、これはファイルの読み書き時にデータをコピーすることで動作します。しかし、virtiofs DAX と呼ばれる特別なモードがあり、ゲストがホストのファイルを直接メモリにマップすることができます！ファイルシステムが DAX で共有されマウントされると、ホストとゲストで同じファイルの mmap() が実際に直接メモリを共有し、アトミック操作がゲストとホストの間で動作します！なんてクールなんでしょう？

結局のところ、DAXはちょっと新しくて、ちょっと壊れてるので、libkrunとゲストカーネルの両方にいくつかのパッチを送って、確実に動くようにする必要がありました（そのうちのいくつかをアップストリームする方法をまだ考え中です...）。muvm は現在、DAX サポートを使ってゲストの /dev/shm をマウントすることで、ホストとゲストが POSIX 共有メモリを共有できるようにしています。

しかし...libxshmfence はまだ memfds の使用を主張しています。どうすれば直るんだろう？

chaos_princess はその名前に忠実に、ptrace という解決策で完全なカオスモードに入ることにした！muvm-x11bridge がクライアントプロセスから memfd を受け取ると、それを ptrace し、必要なシステムコールを注入して /dev/shm ファイルをオープンし、何事もなかったかのようにファイルディスクリプタをスワップアウトします。その後、ファイル名をホストに渡すだけで、libkrun はそれを開き、何事もなかったかのように fd を X サーバーに渡すことができる。

それから、muvm-x11bridge は小さな futex ウォッチャースレッドを実行し、クライアントプロセスからの futex() ウェイクアップコールを検出し、ウェイクアップをホストに送信します。

狂気の沙汰だ！でもうまくいった...ような？ptrace()ダンスは非常に繊細で、うまく動作させるのに何度も試行錯誤した。futex()プロキシもまた、正しく動作させるのがかなり難しいことが判明した。最終的にはかなり確実に動作するようになったが、重大な制限がある： muvm-x11bridgeが壊れてしまうので、X11クライアントアプリでptraceを他の目的で使うことはできません。つまり、デバッグのためにstraceを使うことができず、ptraceを好んで使うような複雑なアプリ（ChromiumやChromiumベースのアプリなど）では問題が発生します。

LD_PRELOAD を使って libxshmfence をハイジャックすることも考えられるが、これも chaos_princess が実装した解決策だ...しかし、3つのアーキテクチャ（ネイティブの arm64、エミュレートされた x86、エミュレートされた x86-64）にまたがってこの作業を行うので、3つのバージョンのライブラリを出荷しなければならない。

本気でこれを出荷したいのであれば、もっと良いソリューションが必要だった...。

## Futex、テイク2

libxshmfenceのコードをよく見てみると、memfd()が動作しない場合の[フォールバックパスがいくつかあること](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/blob/libxshmfence-1.3.2/src/xshmfence_alloc.c?ref_type=tags#L82)がわかった。まず、shm_open(SHM_ANON, ...)を使うが、これはBSDのもので、Linuxでは使えない。次に、O_TMPFILE で open() を試みますが、O_TMPFILE は virtiofs ではサポートされていません。最後に、/dev/shmにファイルをオープンしてリンクを解除するという、昔ながらの方法で実行します。memfd()を使わないようにできれば、まさに私たちが望むことができるでしょう。できるだろうか？

[seccomp-bpf](https://www.kernel.org/doc/html/v5.1/userspace-api/seccomp_filter.html)フィルターを使ってmemfd()システムコールを無効にするなど、いくつか試してみた。残念ながら、これはFEXのコードパスのように、memfd()が存在し、動作していると仮定しているものを壊してしまう。

それで...結局、環境変数を使って memfd() の使用を無効にできるよう、libxshmfence にちょっとした[マージリクエスト](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/merge_requests/9)を送っただけなんだけど...それがマージされた！

このパッチはまだ新しい libxshmfence バージョンとしてリリースされていないので、新しい muvm バージョンを出荷したいディストリビューションは、信頼性の低い ptrace() ハックに逆戻りしないように、このパッチを libxshmfence パッケージにバックポートする必要がある。

```
ありがたいことに、これらのフェンスは常にX11クライアント上で作成され、Xサーバーに共有されるので、ゲストのX11クライアントだけがハックを必要とし、Xサーバーは余分な環境変数やパッチなしでlibxshmfenceを使うことができます。
```

これで完了でしょうか？libxshmfence はまだ共有メモリファイルを削除し、fd だけを X11 サーバーに渡します。これは望ましいことだし（エラー時に古いファイルが残るリスクを回避できる）、libxshmfenceにこれ以上ハックを増やしたくないからだ...では、ファイルをリオープンできない場合、どうやってホスト側にfdを渡せばいいのだろう？

さて...考えてみると、たとえファイルが削除されたとしても、ゲストで開いているすべての fd は実際にはホストの VMM によって開かれています。これが virtiofs の仕組みです！つまり、ホストはすでに必要なファイルディスクリプタをどこかに持っている...それを見つけるだけでいいのだ。これを動作させるために、libkrunのvirtiofs実装に魔法のfd-export ioctl()を追加しました。これは GPU バッファのエクスポートのようなもので、X11 クロスドメインプロトコルの一部としてホストに渡せる識別子を与えます。これはvirtiofsコードとrutabaga_gfxクロスドメインコードの間でexported-fdテーブルを共有することで実装されています。

```
このトリックは、複数の独立したクロスドメイン・クライアント（おそらく異なるユーザーとして実行されている）が互いのFDを盗む可能性があるという意味ではセキュアではないが、いずれにせよmuvm-x11bridgeのインスタンスは1つしかなく、microVM全体が単一ユーザーのパーミッションのコンテキストで実行されるため、私たちのユースケースには影響しない。それに、virtgpu cross-domainが今日意味のあるパーミッションチェックをするとは思えない。
```



最終的に、futex とクロスドメインの X11 コードのデバッグを重ねた後、ようやく安定し、マージしてリリースする準備ができました！

## Waylandについてはどうですか？

将来的には、ネイティブのWaylandパススルーもサポートしたいと考えています。そのためには muvm-x11bridge と対になる Wayland が必要です。sommelierと違って、これはずっとシンプルで同じ哲学に従う予定であり、X11パススルーと同じように動作すると期待しています。しかし、フェンシングと明示的な同期を先に解決すべきなので、純粋なWaylandパススルーは...今のところToDoリストに残るだろう。良いニュースは、奇妙なfutexのようなものがないことだ！

## Muvm、リロード

今日、sudo dnf update --refreshでFedora Asahi Remixシステムをアップデートすると、x86エミュレーションスタックが10月のリリースより改善されます：

### muvm の変更

- `muvm-x11bridge` で直接X11パススルー（`-sommelier`でsommelierに戻せるが、なぜそうする？）
  - Steamやその他のウィンドウアプリが正しく動作するようになり、システムトレイアイコンも動作するようになりました。
  - また、ホスト上で正しく設定されていれば、古き良きXIMを使ってSteamで日中韓の入力メソッドを使うこともできます！(KDE Plasma 上の fcitx でテスト)
- `muvm-hidpipe` が統合されたので、私たちの`steam`ラッパーを使わなくてもゲームパッドが動くようになりました。実際、Steam を一度インストールすれば、`muvm -- FEXBash -c '~/.local/share/Steam/steam.sh'` で機能を損なうことなく実行できる。
- `/dev/shm` がホストとゲストで共有されるようになり、興味深い使用例が可能になりました。
- ゲストのメモリがより適切に管理されるようになり、ホストの OOM が減るはずです。
- 8GB マシンのデフォルトのメモリ割り当てが高くなり、より多くのゲームがプレイできるようになりました。
- すでに稼働しているVMの中でコマンドを実行するインタラクティブモード。例えば、`muvm -ti -- bash` を実行するとシェルが起動
- また、root-serverポートに接続してrootとしてコマンドを実行することもできます： `muvm -tip 3335 -- bash`
- 小さな改善と修正がたくさんある。

### FEX の変更
- 新しい Steam CEF アップデートの修正を含む、[多くのバグフィックスと改善](https://fex-emu.com/FEX-2412/)

### virglrendererとMesaの変更
- [Vulkan 1.4 をサポート](https://social.treehouse.systems/@AsahiLinux/113584784231664115)
- 多くのバグフィックスとパフォーマンスの改善

### Steamラッパーの変更
- SteamがBig Pictureモードではなく、通常のウィンドウモードで起動するようになりました。

![steam im](https://asahilinux.org/img/blog/2024/12/steam-im.png)
日本語入力が可能な標準ウィンドウUIのSteam

また、Steam 以外のアプリケーションを実行するために必要なビットのインストールも自動化されているので、Steam を実行したくない場合は、Steam パッケージは必要ありません。上記のようにまずシステムをアップデートし、それから `sudo dnf install fex-emu` をインストールするだけで、`muvm` で x86 および x86-64 アプリケーションを実行するのに必要なものがすべて手に入ります。

ウィンドウ管理がそれなりにうまくいくようになったので、ゲーム以外のアプリも試してみることをお勧めする。一般に、スタンドアロンの tarball としてパッケージ化されたアプリケーション（複雑な OS 依存関係や他のアプリケーションとの相互作用がないもの）は、AppImages を含めてうまく動作する可能性が高いです。FlatpakはFEX/muvmと統合されていないので、x86-64のFlatpakはまだ動作しないことに注意してください。どの程度動作するか、ぜひ教えてください！

#### Asahi Lina · 2024-12-12
