[Beyond Gaming: X11 bridging in muvm](https://asahilinux.org/2024/12/muvm-x11-bridging/)の非公式日本語訳です。

訳注: 本家ブログへのリンクは対応する日本語訳へのリンクに変更

---
# ゲームを超えて: muvmによる X11 ブリッジ
- [前回](https://github.com/asfdrwe/asahi-linux-translations/blob/main/PROGRESS202410.md)

みなさん、こんにちは！Asahi Linuxのx86/x86-64エミュレーション・スタックにとてもクールなアップデートがありましたので、
その内容をシェアしたいと思います。今日、ゲーム以外のアプリでも使えるようになりました！

![cisco-pt](https://asahilinux.org/img/blog/2024/12/cisco-pt.png)

Fedora Asahi Remix上で動作するCisco Packet Tracer

## VM 上のネイティブグラフィックス

[以前のブログ記事](https://github.com/asfdrwe/asahi-linux-translations/blob/main/PROGRESS202410.md)を覚えているかもしれませんが、
Asahi Linux は [muvm](https://github.com/AsahiLinux/muvm) が駆動する microVM ですべての x86/x86-64 アプリケーションを実行しています。
ハードウェア GPU パススルーなしにネイティブに近い性能で実際の VM 上でゲームを実行するにはどうすればいいのでしょうか？

AMD/Intel　システムでは（そして実際、Apple Silicon 上の macOS でも）、GPU 仮想化は常にある種の制限を受けてきました。
選択肢は、ハードウェア GPU を完全にパススルーするか、API レベルの GPU 準仮想化(paravertualization)を使用するかです。ハードウェア GPU を
パススルーすることで GPU 機器を完全にゲストに割り当てます。つまり、『本物』の GPU 機器のように見えることを意味します。これはその GPU 用の
本物のドライバを持つすべてのゲスト OS上で動作し、ネイティブな性能になりますが、GPU　がゲスト専用であることを意味し、ホストと共有したり、ゲストと
ホストのウィンドウを1つの画面に統合したりすることはできません。一方、API レベルの GPU 準仮想化は、基本的に OpenGL や Vulkan や Metalコマンドを
ゲストからホストに送信し、ホストの GPU ドライバスタックによって処理されます。これはゲストに 『汎用(generic)』準仮想化 GPU ドライバを必要とし、
すべての高レベル GPU 描画コマンドがゲストからホストへの障壁を越え、ホスト上で処理されなければならないため、ネイティブ GPU の使用よりもはるかに遅くなります。
これが macOS での GPU 仮想化の仕組みです。一部の GPU(例えば最近の Nvidia GPU)はハードウェア仮想化サポートで真の共有に対応していますが、これは
まだ上流にはなく、ハードウェア対応が必要で、Apple GPU にはありません。

しかし、もっと良い方法があるとしたら？判明しました！[DRM Native Context](https://indico.freedesktop.org/event/2/contributions/53/attachments/76/121/XDC2022_%20virtgpu%20drm%20native%20context.pdf)です。

コンセプトはとてもシンプルです： ゲスト上で GPU ドライバスタック全体を実行しハードウェア GPU 全体を通過させる代わりに（共有なし）、もしくはホスト上で 
GPU ドライバスタック全体を実行し、高レベル API を通過させる代わりに（遅い）、真ん中で分割するのはどうでしょうか？DRM Native Context は、
ホスト上で GPU *カーネルドライバ*を実行し、ゲスト上で GPU *ユーザースペースドライバ*（Mesa）を実行し、ゲストからホストへ*カーネル UAPI インターフェイス*にパススルーします。

これで両方の長所を生かすことができます。GPU ユーザースペースドライバはゲストで実行されるため、あたかも本物のハードウェア GPU を駆動しているかのように、
フルパフォーマンスで実行することができます。また、GPU カーネルドライバはホストで実行されるため、ホストアプリケーションとゲストアプリケーションの間で
共有することができます。単純化したレンダリング操作は次のようになります：

1. アプリケーションは、GL または Vulkan の描画コマンドを発行して画面に描画。ゲームでは 1 フレームあたり何千もの描画コマンドを実行する可能性あり
2. ゲストのユーザー空間 Mesa ドライバはこれらのコマンドを受け取りGPU コマンド構造に変換。これらのコマンド構造は virtio-gpu プロトコルを使用してドライバがホストから事前に直接マッピングした *GPU メモリに直接書き込まれる*ため、ネイティブで実行するのと同速度
3. アプリケーションは描画ストリームをフラッシュするか、GPU にレンダリング開始を要求
4. ユーザー空間の Mesa ドライバは、ホスト上でネイティブに実行する場合と同じように、全体的なレンダリング情報を準備
5. 次に Mesa はコマンドを virtio-gpu *実行バッファ*にラップし、コマンドをホストにパススルーするのに必要な情報を保持
6. コマンドはせいぜい1～2キロバイトのデータで、ゲストカーネルの virtio-gpu ドライバを経由してホストの仮想マシンモニタに入力
7. VMM はコマンドを受け取って [virglrenderer](https://gitlab.freedesktop.org/asahi/virglrenderer) に送り、virglrenderer はネイティブの UAPI GPU コマンド構造をアンラップ
8. virglrenderer は `ioctl()` を発行してホスト GPU カーネルドライバにコマンドに送る
9. ホスト GPU カーネルドライバはコマンドを GPU に送信

ホスト上でネイティブにレンダリングするプロセスは、ステップ 5 から 7 を削除する以外はまったく同じです（ステップ 8 では、Mesa は `ioctl()` を直接発行します）。
したがって、オーバーヘッドは非常に小さくなります。GL または Vulkan コマンドストリーム全体を通過させる代わりに、ゲストからホストへの余分な通信はわずかな
データ量にしか存在しないからです。また、ネイティブとまったく同じドライバを実行できるため、GPU ドライバの品質と互換性を維持したまま、GPU の機能とまったく同一にできます。

セキュリティと分離についてはどうでしょうか？ゲスト上の各プロセスは、ホストカーネルドライバの観点からは独立した GPU プロセスとして扱われます
（より具体的には、ゲストプロセスが仮想 GPU デバイスノードを開くたびに、ホスト VMM はそのために実際の GPU デバイスノードを別のファイル記述子として開きます）。
つまり、ゲスト GPU プロセスは、ネイティブ GPU プロセスが互いに隔離されているのと同じように隔離されています。

今日のところ、DRM Native Contextは [freedreno](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/14900) の上流にしかありません！
しかし、[Intel](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/29870) と [AMD](https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/21658) 
GPU 向けの PR が進行中であり、これは Linux ホスト上の Linux ゲスト向けの GPU 仮想化の未来だと本当に思っています(そしておそらくいつの日か、Windows 上で Mesa を実行する
Windows ゲストでも）！

10月のゲーム・アップデートでDRM Native Contextを公開しました。

では、何か問題がありますか？ご想像の通り、性能は*全く*ネイティブではありません。性能への影響は*同期(synchronization)*に起因します。GPU　へのコマンド送信には
少々時間がかかり、GPUコマンドの完了を待つのにも少々時間がかかります。一度にたくさんのGPU作業を 『投げっぱなす(fire-and-forget)』アプリケーションは、
ほぼネイティブな性能を示しますが、GPU作業が完了するのを頻繁に待つアプリケーションは、待ち時間の増加とスループットの低下に悩まされます。複数のGPUキューを使用し、
それらの間で同期をとるアプリケーションも、VM ゲストを介して同期が行われるため、現在苦しんではいますが、GPU 間の同期をホストカーネルに移行することで、
ネイティブアプリケーションと同様にこれを改善する予定です（まだ実装していないだけですよ）。

しかし、ゲストで GPU レンダリングを動作させるのは話の半分に過ぎません…

## ウィンドウシステム攻撃
DRM Native Context は自身でゲストを GPU にレンダリングさせますが、ディスプレイについてはまだ話していません！伝統的な VM のように仮想スクリーンデバイスを使って
ゲストをウィンドウの中に独立したデスクトップとして表示させることは可能でしょうが、私たちのスタックでは必要とすることではありません。VM は薄い層にし、アプリケーションは
ホストのウィンドウシステム上にネイティブに表示されるようにしたいのです。

muvm と libkrun のグラフィック対応がコードを共有する ChromeOS の VMM である Crosvm は、*cross-domain contexts*と呼ばれる機能を使ってこれを実現します。
virtio-gpu はゲストに通信チャネルを開かせ、ホスト VMM はそれをホストコンポジタに転送することが可能です。Crosvm　はこれを使って、*sommelier*　というゲストの　Wayland　
コンポジターを使って[Wayland 転送（Wayland Forwarding)](https://crosvm.dev/book/devices/wayland.html)を実装します。sommelier はゲストの Wayland ソケットを
リッスンし、リクエストをホストの Wayland コンポジタに転送します。プロトコルを転送するだけでは不十分です（それ以外は、virtio vsock や TCP などを使って実現できます）。
sommelier は Wayland コマンドストリームの一部として渡されるバッファファイルディスクリプターも抽出し、virtio-gpu メカニズムを使ってホストとゲスト間で*直接*共有する
必要があります。これによりウィンドウフレームバッファが共有され、余分なコピーコストなしにホスト上で合成できるようになります。ホスト VMM 側では、
これは[rutabaga_gfx](https://github.com/containers/libkrun/tree/v1.9.5/src/rutabaga_gfx/src/cross_domain)のクロスドメインコンポーネントによって処理されていて、
Wayland プロトコルを転送し共有バッファをホストWaylandコンポジターが理解できる fds(訳注: おそらくファイルディスクリプタ) に変換するコマンドを実装します。

ひとつだけ余分な問題がありました。同期です。ゲスト上の GPU アプリがバッファにレンダリングするとき、レンダリングの終了を待つ前にそれを直接ホストに渡します。
*暗黙的同期（implicit sync）*と呼ばれるメカニズムが、カーネルに同期を直接処理させることになっています。カーネルは、バッファ上でどの GPU 操作が保留されているかを知っており、
別のコンテキストでバッファから読み込む前に、それらが完了するまでGPUを待たせます。暗黙的同期はレガシー機能と考えられているため、私たちの GPU カーネルドライバはこれを実装せず、
互換性のために Mesa で実装しています（Nvidia はこれを拒否しており、これが Wayland が Nvidia GPU 上で長い間壊れていた理由です…）。私たちの Mesa ドライバは、
共有バッファがレンダーターゲットであるときに暗黙の同期情報（*GPU fence*）を共有バッファに自動的に挿入し、バッファから読み出す前にもう一方の fence を取り出します。
これはホストとゲストの両方でうまくいきましたが、ホストとゲストの間で共有されているバッファではうまくいきませんでした。ゲスト Mesa がゲストバッファに fence を挿入すると、
その fence はゲストカーネルにのみ登録されるため、ホストカーネルは進行中のレンダリングについて何も知らず、必要なときにホスト Mesa に fence を引き渡すことができませんでした。
そのため、ティアリングや視覚的な不具合や『フレームラグ』が発生しました。これを解決するために、結局、暗黙的同期に戻しました。 Asahi virtgpu プロトコルは、『古典的な』
暗黙的同期ドライバと同じようにサブミッションを持つバッファリストを含み、それらのバッファはホスト VMM の virglrenderer コードで fence が登録されます。
暗黙的同期互換性コードは、すべてのベースをカバーするために、ゲスト Mesa とホスト VMM で2回実行されます。将来的には、明示的同期(explicit sync)を適切に実装し、
クロスドメインのものと統合し、このハックを削除する予定ですが、上流には存在しないゲストカーネルのパッチが必要になるため、少々お待ちください。話がそれました。

X11 についてはどうでしょう？ほとんどのゲームは X11 を必要とするので、sommelier はゲスト内で独自の *XWayland* インスタンスを実行して X11 対応を提供できます。簡単ですよね？

この解決策を10月に公開したのですが… あまりうまくいきませんでした。

## sommelier の悩み
結局のところ、sommelier は我々が期待していたほど素晴らしいソリューションではありません。これは ChromeOS ホストで使うように設計されており、より
一般的なLinuxデスクトップではあまりうまく動作しません。

sommelier は*多すぎるの*と*少なすぎるの*の両方です。非常に複雑なコードでできていて、Wayland プロトコルの解析やバッファのコピーなどをたくさんのことを行います。
つまり、ゲスト上のアプリケーションは、ホスト上でネイティブに動作するアプリケーションのようには動作しません。DPI のスケーリングやウィンドウ/ポップアップの配置など、
多くの問題がありました。同時に、sommelier は XWayland と統合するのに十分なことをしていません。KDE Plasma のような一般的な Wayland デスクトップ環境では、
コンポジタは X11 アプリケーションと適切に統合するために多くの余分な作業を行い、XWayland と直接会話してそれらを管理しています。これは sommelier ではこれらの
作業を行っておらず素の X11 統合しか提供していません。X11 アプリにはタイトルバーがなく、メニューは正しく機能せず、クリップボードは正しく機能せず、システムトレイとの
統合はなく、いくつかのウィンドウは奇妙に透けて表示されます…

その上、sommelierはCで書かれており、クライアント接続を完全に分離していない。一般的にはXWaylandクライアントで十分に動作するが、おかしなことがあるとクラッシュしてVM環境全体がダウンすることもある。ネイティブのWaylandプロキシは問題外でした： 試してみましたが、壊れすぎていてまったく使えませんでした。

そこで私たちは、sommelierとXWaylandをX11のみのソリューション（Waylandソケットをサポートしない）として出荷し、フルスクリーンアプリは一般的に十分に機能するため、ゲームのみのソリューションとして宣伝することにしました。これは、Steamをビッグピクチャーモードで動かすことを意味した（ウィンドウモードはかなり壊れていたため）。また、ウィンドウ・ランチャーを使うゲームは使えないことが多かった。

しかし、私たちはより良いソリューションを必要としていました。

## X11が先か？

muvmがmuvmと呼ばれるようになる前に、virtioのvsockソケットでX11を直接転送する実験をしたことがある。これはネットワークやSSHトンネル経由でX11を実行するのと同じように動作します。この欠点は、GPUアクセラレーションで動作しないことだ。このような「ダム」ソケット上でGPUバッファを通過させる方法がないからだ。それでも、ソフトウェアレンダリングを強制しながらいくつかのアプリを動かしてみると、ゲーム以外のアプリではこちらの方がはるかに優れたソリューションであることは明らかだった。X11アプリはホスト上とまったく同じように動作し、ウィンドウ管理の問題もなく、クリップボードやトレイの統合なども適切に動作した。このままゲーム以外のアプリの代替品として出荷する覚悟はできていたが、GPUアクセラレーションを動作させるのがどれだけ大変なことか......と思っていたら、chaos_princessがx112virtgpuというのを出してきた。

```
Waylandが未来なのに、なぜX11に注目するのか？端的に言えば、人々が実行したいと思うx86/x86-64アプリのほとんどは、
Waylandサポート用にビルドされていないか、それ以前のものだからだ。Asahi Linux上で動作するネイティブ・デスクトップ
環境ではWaylandを全面的に採用していますが、XWaylandはまだ完全にサポートされていますし、エミュレーション下で
動作するゲームやレガシー・アプリケーションでは、X11プロトコルのサポートを優先するのが賢いやり方です。
私たちが10月に出荷したソリューションは、Waylandを使用しているにもかかわらず、Waylandアプリケーションを
サポートしませんでした（私たちはそれを有効にしようとしましたが、本当に壊れていました）。
```

x112virtgpuは現在muvm-x11bridgeに改名され、まさにsommelierがそうあって欲しかったものだ： X11 プロトコルのシンプロキシで、クロスドメインチャンネルを使用して、virtgpu バッファ共有を使ってフレームバッファをコピーせずにホスト X サーバに直接転送します。sommelier とは異なり、特別な処理が必要なコマンドを抽出するために必要以上に X11 プロトコルを解釈しようとしません。つまり、直接X11パススルーするのと同じように動作し、さらにGPUアクセラレーションとバッファ共有も行います。

......あるいは、それがとにかく理論的なことだ。しかし、1つだけ重大な問題が...。

## Futexって何？

X11プロトコルはかなり特殊だ。ファイルディスクリプタを使ってフレームバッファを通過させるだけでなく、フェンスという別のものも通過させるのだ。いや、最新のGPUの明示的同期フェンス（ごく最近のX11ではそれもサポートされているが、それはまた別の話）ではない。CPU側のフェンスは、フテックスを使って実装されている。

[Futex](https://en.wikipedia.org/wiki/Futex)は、Linux上でクロススレッドおよびクロスプロセス同期を行うためのカーネル・システムコールである。基本的には、自分でミューテックスを作成するためのプリミティブである。2つのスレッドまたはプロセスは、メモリを直接共有し、アトミックを使用して同期することになっている。あるプロセスが他のプロセスを待つ必要があるときは、カーネルのfutex()システムコールを使って自分自身をスリープ状態にし、他のプロセスがスリープ状態を解除する必要があるときは、同じシステムコールを使ってスリープ状態にする。カーネルは、システムコールが共有メモリの実際の値と同期していることを確認するので、競合状態は発生しない。

しかし、クロスドメインのfutex()システムコールというものは存在しない。ホストとゲストのカーネルは、お互いのfutexについて何も知らない。そして、これを動作させるためには、メモリを共有する必要がある。

X11プロトコルをもっと解釈して、futexフェンスが何のために使われるのかを正確に理解することで、この問題を高レベルで解決することができる。ホストVMMとx11bridgeの各サイドは、どちらのサイドがfutexのシグナリングと待機を担当するかを理解しなければならない。そして、シグナルを送る側のプロキシコンポーネントはシグナルを待ち、クロスドメインコマンドを送信してシグナルをもう一方の側に転送しなければならない。これは機能するだろうが、手間がかかるし、余計な待ち時間が増えることになる： futexの仕組みは、一方がもう一方を待っていなければ、アトミック操作を直接使うことで、システムコールを一切使わずに動作することになっている。

そこでchaos_princessは、別のことを試してみることにした。ホストプロセスとゲストプロセスが、共有メモリバッファを実際に一緒にマッピングすることができれば、あたかも同じ側で実行されているかのようにアトミックを使うことができ、futex()システムコールの使い方を何らかの方法でプロキシするだけでよい。そうすれば、futex()システムコールのプロキシを使えばいいだけだ。GPUバッファを使うという手もあるが、ちょっとやりすぎな気がするし、Xサーバーやクライアントと直接統合するのは無理がある...。

X11の共有メモリフェンスを扱うライブラリはlibxshmfenceです。このライブラリは、メモリを共有するために複数のメカニズムを使うことができる。Linuxの場合、[デフォルトはmemfd()](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/blob/libxshmfence-1.3.2/src/xshmfence_alloc.c?ref_type=tags#L78)で、これは「メモリ内」ファイルを作成するシステムコールだ。VMの境界を越えて、これらのファイルを共有できるのだろうか？残念ながら、簡単ではない。

しかし、別の方法がある。

muvmでは、すでにホストのファイルシステムを直接ゲストと共有している。これは基本的に、VMにプロキシされたFUSEファイルシステムです。通常、これはファイルの読み書き時にデータをコピーすることで動作します。しかし、virtiofs DAX と呼ばれる特別なモードがあり、ゲストがホストのファイルを直接メモリにマップすることができます！ファイルシステムが DAX で共有されマウントされると、ホストとゲストで同じファイルの mmap() が実際に直接メモリを共有し、アトミック操作がゲストとホストの間で動作します！なんてクールなんでしょう？

結局のところ、DAXはちょっと新しくて、ちょっと壊れてるので、libkrunとゲストカーネルの両方にいくつかのパッチを送って、確実に動くようにする必要がありました（そのうちのいくつかをアップストリームする方法をまだ考え中です...）。muvm は現在、DAX サポートを使ってゲストの /dev/shm をマウントすることで、ホストとゲストが POSIX 共有メモリを共有できるようにしています。

しかし...libxshmfence はまだ memfds の使用を主張しています。どうすれば直るんだろう？

chaos_princess はその名前に忠実に、ptrace という解決策で完全なカオスモードに入ることにした！muvm-x11bridge がクライアントプロセスから memfd を受け取ると、それを ptrace し、必要なシステムコールを注入して /dev/shm ファイルをオープンし、何事もなかったかのようにファイルディスクリプタをスワップアウトします。その後、ファイル名をホストに渡すだけで、libkrun はそれを開き、何事もなかったかのように fd を X サーバーに渡すことができる。

それから、muvm-x11bridge は小さな futex ウォッチャースレッドを実行し、クライアントプロセスからの futex() ウェイクアップコールを検出し、ウェイクアップをホストに送信します。

狂気の沙汰だ！でもうまくいった...ような？ptrace()ダンスは非常に繊細で、うまく動作させるのに何度も試行錯誤した。futex()プロキシもまた、正しく動作させるのがかなり難しいことが判明した。最終的にはかなり確実に動作するようになったが、重大な制限がある： muvm-x11bridgeが壊れてしまうので、X11クライアントアプリでptraceを他の目的で使うことはできません。つまり、デバッグのためにstraceを使うことができず、ptraceを好んで使うような複雑なアプリ（ChromiumやChromiumベースのアプリなど）では問題が発生します。

LD_PRELOAD を使って libxshmfence をハイジャックすることも考えられるが、これも chaos_princess が実装した解決策だ...しかし、3つのアーキテクチャ（ネイティブの arm64、エミュレートされた x86、エミュレートされた x86-64）にまたがってこの作業を行うので、3つのバージョンのライブラリを出荷しなければならない。

本気でこれを出荷したいのであれば、もっと良いソリューションが必要だった...。

## Futex、テイク2

libxshmfenceのコードをよく見てみると、memfd()が動作しない場合の[フォールバックパスがいくつかあること](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/blob/libxshmfence-1.3.2/src/xshmfence_alloc.c?ref_type=tags#L82)がわかった。まず、shm_open(SHM_ANON, ...)を使うが、これはBSDのもので、Linuxでは使えない。次に、O_TMPFILE で open() を試みますが、O_TMPFILE は virtiofs ではサポートされていません。最後に、/dev/shmにファイルをオープンしてリンクを解除するという、昔ながらの方法で実行します。memfd()を使わないようにできれば、まさに私たちが望むことができるでしょう。できるだろうか？

[seccomp-bpf](https://www.kernel.org/doc/html/v5.1/userspace-api/seccomp_filter.html)フィルターを使ってmemfd()システムコールを無効にするなど、いくつか試してみた。残念ながら、これはFEXのコードパスのように、memfd()が存在し、動作していると仮定しているものを壊してしまう。

それで...結局、環境変数を使って memfd() の使用を無効にできるよう、libxshmfence にちょっとした[マージリクエスト](https://gitlab.freedesktop.org/xorg/lib/libxshmfence/-/merge_requests/9)を送っただけなんだけど...それがマージされた！

このパッチはまだ新しい libxshmfence バージョンとしてリリースされていないので、新しい muvm バージョンを出荷したいディストリビューションは、信頼性の低い ptrace() ハックに逆戻りしないように、このパッチを libxshmfence パッケージにバックポートする必要がある。

```
ありがたいことに、これらのフェンスは常にX11クライアント上で作成され、Xサーバーに共有されるので、ゲストのX11クライアントだけがハックを必要とし、Xサーバーは余分な環境変数やパッチなしでlibxshmfenceを使うことができます。
```

これで完了でしょうか？libxshmfence はまだ共有メモリファイルを削除し、fd だけを X11 サーバーに渡します。これは望ましいことだし（エラー時に古いファイルが残るリスクを回避できる）、libxshmfenceにこれ以上ハックを増やしたくないからだ...では、ファイルをリオープンできない場合、どうやってホスト側にfdを渡せばいいのだろう？

さて...考えてみると、たとえファイルが削除されたとしても、ゲストで開いているすべての fd は実際にはホストの VMM によって開かれています。これが virtiofs の仕組みです！つまり、ホストはすでに必要なファイルディスクリプタをどこかに持っている...それを見つけるだけでいいのだ。これを動作させるために、libkrunのvirtiofs実装に魔法のfd-export ioctl()を追加しました。これは GPU バッファのエクスポートのようなもので、X11 クロスドメインプロトコルの一部としてホストに渡せる識別子を与えます。これはvirtiofsコードとrutabaga_gfxクロスドメインコードの間でexported-fdテーブルを共有することで実装されています。

```
このトリックは、複数の独立したクロスドメイン・クライアント（おそらく異なるユーザーとして実行されている）が互いのFDを盗む可能性があるという意味ではセキュアではないが、いずれにせよmuvm-x11bridgeのインスタンスは1つしかなく、microVM全体が単一ユーザーのパーミッションのコンテキストで実行されるため、私たちのユースケースには影響しない。それに、virtgpu cross-domainが今日意味のあるパーミッションチェックをするとは思えない。
```



最終的に、futex とクロスドメインの X11 コードのデバッグを重ねた後、ようやく安定し、マージしてリリースする準備ができました！

## Waylandについてはどうですか？

将来的には、ネイティブのWaylandパススルーもサポートしたいと考えています。そのためには muvm-x11bridge と対になる Wayland が必要です。sommelierと違って、これはずっとシンプルで同じ哲学に従う予定であり、X11パススルーと同じように動作すると期待しています。しかし、フェンシングと明示的な同期を先に解決すべきなので、純粋なWaylandパススルーは...今のところToDoリストに残るだろう。良いニュースは、奇妙なfutexのようなものがないことだ！

## Muvm、リロード

今日、sudo dnf update --refreshでFedora Asahi Remixシステムをアップデートすると、x86エミュレーションスタックが10月のリリースより改善されます：

### muvm の変更

- `muvm-x11bridge` で直接X11パススルー（`-sommelier`でsommelierに戻せるが、なぜそうする？）
  - Steamやその他のウィンドウアプリが正しく動作するようになり、システムトレイアイコンも動作するようになりました。
  - また、ホスト上で正しく設定されていれば、古き良きXIMを使ってSteamで日中韓の入力メソッドを使うこともできます！(KDE Plasma 上の fcitx でテスト)
- `muvm-hidpipe` が統合されたので、私たちの`steam`ラッパーを使わなくてもゲームパッドが動くようになりました。実際、Steam を一度インストールすれば、`muvm -- FEXBash -c '~/.local/share/Steam/steam.sh'` で機能を損なうことなく実行できる。
- `/dev/shm` がホストとゲストで共有されるようになり、興味深い使用例が可能になりました。
- ゲストのメモリがより適切に管理されるようになり、ホストの OOM が減るはずです。
- 8GB マシンのデフォルトのメモリ割り当てが高くなり、より多くのゲームがプレイできるようになりました。
- すでに稼働しているVMの中でコマンドを実行するインタラクティブモード。例えば、`muvm -ti -- bash` を実行するとシェルが起動
- また、root-serverポートに接続してrootとしてコマンドを実行することもできます： `muvm -tip 3335 -- bash`
- 小さな改善と修正がたくさんある。

### FEX の変更
- 新しい Steam CEF アップデートの修正を含む、[多くのバグフィックスと改善](https://fex-emu.com/FEX-2412/)

### virglrendererとMesaの変更
- [Vulkan 1.4 をサポート](https://social.treehouse.systems/@AsahiLinux/113584784231664115)
- 多くのバグフィックスとパフォーマンスの改善

### Steamラッパーの変更
- SteamがBig Pictureモードではなく、通常のウィンドウモードで起動するようになりました。

![steam im](https://asahilinux.org/img/blog/2024/12/steam-im.png)
日本語入力が可能な標準ウィンドウUIのSteam

また、Steam 以外のアプリケーションを実行するために必要なビットのインストールも自動化されているので、Steam を実行したくない場合は、Steam パッケージは必要ありません。上記のようにまずシステムをアップデートし、それから `sudo dnf install fex-emu` をインストールするだけで、`muvm` で x86 および x86-64 アプリケーションを実行するのに必要なものがすべて手に入ります。

ウィンドウ管理がそれなりにうまくいくようになったので、ゲーム以外のアプリも試してみることをお勧めする。一般に、スタンドアロンの tarball としてパッケージ化されたアプリケーション（複雑な OS 依存関係や他のアプリケーションとの相互作用がないもの）は、AppImages を含めてうまく動作する可能性が高いです。FlatpakはFEX/muvmと統合されていないので、x86-64のFlatpakはまだ動作しないことに注意してください。どの程度動作するか、ぜひ教えてください！

#### Asahi Lina · 2024-12-12
